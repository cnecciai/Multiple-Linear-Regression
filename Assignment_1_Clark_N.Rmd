---
title: "Capital Bike Share Predictive Model Report"
author: "Prepared By: Clark P. Necciai Jr."
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    number_sections: yes
    toc_depth: '3'
fontsize: 12pt
geometry: margin=1in
header-includes:
- \usepackage{setspace}
- \singlespacing
linkcolor: blue
---

```{r setup, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
#Default Setup
knitr::opts_chunk$set(include = FALSE, echo = FALSE, tidy=TRUE, cache=FALSE, message=FALSE, warning=FALSE,tidy.opts=list(width.cutoff=55, keep.blank.line=FALSE),fig.align='center', fig.width=3.50, fig.height=2.75, dpi = 300)
```

\pagebreak

```{r}
library(visdat)
library(car)
library(corrplot)
library(e1071)
library(tidyverse)
library(ggpubr)
library(DT)
#Set default theme for ggplot
theme_set(theme_minimal()) 
```

# Executive Summary

|       Capital Bike Share provides a network of bicycles to the denizens of the Washington D.C. region. We were approached by Capital Bike Share to delve into their dataset observations across 2011 and 2012. Preliminary insights revealed that rental demand can be affected by a variety of influential factors.

|        We examined multi-variable trends and deliberated on insightful patterns. Based on our early findings, we suspected rentals being driven by bicycles being used as a primary mode of transportation during work-commute hours. Furthermore, our resulting model confirmed our preliminary analysis, as it was ultimately determined that hour and temperature variables and were among the most significant factors in accurately predicting bicycle rentals.

# Problem Statement and Approach

Our primary tasks in this analysis were twofold:

-   Identifying the most influential predictive variables in determining total bicycle rentals
-   Fitting a multiple regression model predicting total bicycle rentals

|       Beginning with an exploratory data analysis, we determined variables which had significant relation to the target variable. After a well-documented, granular analysis of 17 features over 17,379 observations, we utilized our findings for comprehensive modeling utilizing stepwise selection to select only the most significant predictors. This process culminated in a multiple linear regression model that not only accurately predicted rental demand, but simultaneously provided assumption-backed diagnostics confirming our models' reliability and generalizability. We then end with our conclusions and recommendations.

# Methodology

## Data Preprocessing

|       We began our approach with an overview of the integrity of our dataset's structure. We discovered no missing values nor duplicated observations~[Appendix-Fig.1]~. We did, however, note variables which had data types that were non-representative of their underlying values.

|        Variables `dteday`, `season`, `yr`, `mnth`, `hr`, `holiday`, `weekday`, `workingday`, and `weathersit` were all considered to have data types and values which were non-representative. We applied more representative data types and applied appropriate labels~[Appendix-Fig.2/3]~. `dteday`, `yr`, `mnth`, `hr`, `weekday`, `weathersit`, and `cnt` were renamed for additional clarity.

```{r}
#Read In Dataset & Initial Look
CBS <- read.csv("Capital Bike Sharing data by hour.csv")
#CBS %>% glimpse()
```

```{r}
#Rename Variables for descriptive clarity
CBS <- CBS %>% rename(date = dteday,
                      year = yr,
                      month = mnth,
                      hour = hr,
                      day = weekday,
                      weather = weathersit,
                      count_rentals = cnt)
CBS <- CBS %>% dplyr::select(-instant)
#CBS %>% glimpse()
```

```{r}
#Visualize for Missing Data
fig.1 <- vis_miss(CBS, sort_miss = T) + 
  labs(y = "NA Values") +
  theme(axis.text.x.bottom = element_text(angle = 60, vjust = 0)) +
  coord_flip()
```

```{r}
#Distinct Row Check - Exclude ID from search
distinctCheck <- CBS[,2:ncol(CBS)]
duplicates <- CBS[which(duplicated(distinctCheck)), ]
if ((nrow(duplicates)) == 0) {
  print("No duplicates detected") }
```

```{r}
fig.2 <- vis_dat(CBS) + theme(axis.title.y = element_blank(),
                     axis.text.x.bottom = element_blank()) +
  scale_fill_brewer(palette = 2)
```

```{r}
#Reevaluate data types
CBS$date <- as.Date(CBS$date)
CBS$season <- factor(CBS$season, labels = c("Winter", "Spring", "Summer", "Fall"))
CBS$year <- factor(CBS$year, labels = c("2011", "2012"))
CBS$month <- factor(CBS$month, labels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                       "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))
CBS$holiday <- as.logical(CBS$holiday)
CBS$day <- factor(CBS$day, labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
CBS$workingday <- as.logical(CBS$workingday)
CBS$weather <- factor(CBS$weather, labels = c("Type 1", "Type 2", "Type 3", "Type 4"))

#Function - Redefine Int Hour Values to Factor Values
reHour <- function(hourInteger) {
  stringHour = ""
  if (hourInteger == 0) {
    return ("12:00AM")
  } else if (hourInteger > 0 & hourInteger < 12) {
    stringHour = paste(hourInteger, ":00AM", sep = "")
  } else if (hourInteger == 12) {
    return ("12:00PM")
  } else {
    hourInteger = hourInteger - 12
    stringHour = paste(hourInteger, ":00PM", sep = "")
  }
  return (stringHour)
}

#Retrieve New Values
newHours <- c()
for (i in 1:nrow(CBS)) {
  newHours <- c(newHours, reHour(CBS$hour[i]))
}
CBS$hour <- factor(newHours)
```

```{r fig.height=4, fig.width=6}
#Visualize final data types in dataset
datatype_plot <- vis_dat(CBS) + theme(axis.title.y = element_blank(),
                     axis.text.x.bottom = element_blank()) +
  scale_fill_brewer(palette = 2)
```

```{r}
### Current Dimensionality of Capital Bike Share Dataset
print(paste("Number of observations/rows: ", dim(CBS)[1]))
print(paste("Number of columns/variables: ", dim(CBS)[2]))
```

## Exploratory Data Analysis (EDA)

### Target Variable

```{r, warning = FALSE}
#Summary Statistics for Continuous Variables
numSummary = function(var, rowname) {
  tab = data.frame(Mean = mean(var) %>% round(2),
           Median = median(var),
           StdErr = sd(var) %>% round(2),
           Skew = skewness(var) %>% round(2),
           Q1 = quantile(var, .25),
           Q3 = quantile(var, .75),
           IQR = IQR(var),
           Min = min(var),
           Max = max(var))
  rownames(tab) = rowname
  return(tab)
}
```

#### `count_rentals`

The variable we aim predict is the total hour-by-hour number of bicycle rentals, `count_rentals`. Our target variable's distribution shows us that `count_rentals` is highly right skewed~[Appendix-Fig.4]~. There is serious variability in the hour-by-hour rentals, with a minimum of a single rental to a maximum of nearly a thousand rentals per hour, but with 50% of our observations being less than the median of **142** rentals. 

```{r}
#Retrieve summary statistics for count_rentals
results = numSummary(CBS$count_rentals, "count_rentals")
#print(results)

#count_rentals distribution
target_plot <- CBS %>% ggplot(aes(count_rentals)) +
  geom_histogram(aes(y = after_stat(density)),fill = "darkblue", color = "black", alpha = 0.6) +
  geom_density(adjust = 2) +
  labs(x = "Total Number of Rentals", y = "Density",
       title = "Distribution of `count_rentals`",
       subtitle = paste("Skewness Value: ", results$Skew),
       caption = "Median Marked By Dashed Red Line") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")
  ) +
  geom_vline(aes(xintercept = results$Median), color = "red", linetype = "dashed")
```



```{r}
# Distinct Values per Variable
#print(paste("Number of Distinct Dates:", n_distinct(CBS$date)))


#Observe Low Frequency of some particular dates
low_count_obs <- CBS %>% group_by(date) %>%
  summarise(count_hour_obs = n()) %>%
  filter(count_hour_obs < 23)

#Expected - Actual
lost_hours = (14*24) - sum(low_count_obs$count_hour_obs)
#print(paste("Hours Lost from Low Count Dates:", lost_hours))
#print(low_count_obs)
```

```{r fig.height=4, fig.width=7}
#Display those date in distribution with fewer counts of hourly observations
date_plot <- CBS %>% group_by(date) %>% 
  mutate(dateCount = n()) %>% 
  filter(dateCount < 23) %>%
  ggplot(mapping = aes(x = fct_reorder((as.character(date)), desc(dateCount)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(date), desc(dateCount)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 2, size = 4.4) +
  scale_fill_viridis_d("Date", option = "magma") +
  guides(fill = "none") +
  labs(x = "Date",
       y = "Number of Observations",
       title = "Dates with Missing Hourly Observations") +
  theme(axis.text.x = element_text(angle = 90),
        plot.caption = element_text(size = 8))
```

```{r fig.width=4.2}
season_plot <- CBS %>% 
  group_by(season)%>%
  mutate(countSeason = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((season), desc(countSeason)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(season), desc(countSeason)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 220, size = 3) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `season`",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
```

```{r fig.width=4}
year_plot <- CBS %>% 
  ggplot(mapping = aes(x = year, fill = year)) +
  geom_bar(color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 500, size = 3) +
  scale_fill_viridis_d("Year", option = "magma") +
  labs(title = "Distribution of `year`",
       x = "",
       y = "") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  guides(fill = "none") +
  coord_flip()
```

```{r fig.width=4.7}
month_plot <- CBS %>% 
  group_by(month) %>%
  mutate(countMonth = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((month), desc(countMonth)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(month), desc(countMonth)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 75, size = 3) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `month`",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
```

```{r fig.width=8}
hours <- CBS$hour %>% unique() %>% as.vector()
hourDF <- CBS %>% 
  group_by(hour) %>% 
  transmute(hourCount = n()) %>% 
  mutate(check = ifelse(hourCount <= 725, TRUE, FALSE))

hour_plot <- hourDF %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours))) +
  geom_bar(color = "black", aes(fill = check), alpha = .4) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 40, size = 3) +
  labs(title = "Distribution of `hour`",
       subtitle = "Slight Drop Trend in Count of Observations During Morning Hours",
       x = "",
       y = "") +
  scale_fill_viridis_d()+
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    plot.caption = element_text(size = 8),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(angle = 85, vjust = 0.6),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none")
```

```{r fig.width=4.5}
day_plot <- CBS %>% 
  group_by(day) %>%
  mutate(countDay = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((day), desc(countDay)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(day), desc(countDay)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 140, size = 3) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `day`",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
```

```{r fig.width=5}
#weather
weather_plot <- CBS %>% 
  group_by(weather) %>%
  mutate(countWeather = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((weather), desc(countWeather)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(weather), desc(countWeather)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 550, size = 3) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `weather`",
       subtitle = "Few Instances of `Type 4` Weather/Inclement Weather Conditions",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    plot.subtitle = element_text(size = 8),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
```

### Predictor Variables

|       Each categorical, continuous, and Boolean features' summary statistics and distributions were investigated. Continuous features considered were `temp`, `atemp`, `hum`, and `windspeed`~[Appendix-Fig.5]~. Boolean features were `holiday` and `workingday`~[Appendix-Pg.15]~. Categorical features were `season`, `year`, `month`, `day`, `hour`, `date`, and `weather`~[Appendix-Fig.6/7]~. 

#### Noteworthy Discoveries

##### `date`

The `date` distribution revealed that not every unique value of date has an expected equivalent number of hourly interval measurements. The majority of each of the **731** distinct date values contained twenty-four or twenty-three of the expected hourly observations. However, fourteen of the dates contained fewer, which in turn ultimately equated to **103** hours worth of missing possible observations~[Appendix-Fig.8]~. 

```{r}
#Retrieve summary statistics for temp
tempSummary = numSummary(CBS$temp, "temp")
numVars = bind_rows(results, tempSummary)
#tempSummary

#temp Distribution
temp_plot <- CBS %>% ggplot(aes(temp)) +
  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = "blue", color = "black", alpha = 0.6) +
  geom_density(adjust = 3) +
  labs(x = "Normalized Celcius",
       y = "Density", 
       title = "Distribution of `temp`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))
```

```{r}
#Retrieve summary statistics for atemp
tempFCSummary = numSummary(CBS$atemp, "atemp")
numVars = bind_rows(numVars, tempFCSummary)
#tempFCSummary

#atemp Distribution
atemp_plot <- CBS %>% ggplot(aes(atemp)) +
  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = "blue", color = "black", alpha = 0.6) +
  geom_density(adjust = 3) +
  labs(x = "Normalized - Real Feel Celcius",
       y = "Density", 
       title = "Distribution of `atemp`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))
```

```{r}
#Retrieve summary statistics for humidity
humSummary = numSummary(CBS$hum, "hum")
numVars = bind_rows(numVars, humSummary)
#humSummary

#Humidity Distribution
hum_plot <- CBS %>% ggplot(aes(hum)) +
  geom_histogram(aes(y = after_stat(density)), fill = "blue", color = "black", alpha = 0.6) +
  geom_density(adjust = 3) +
  labs(x = "Normalized Humidity",
       y = "Density", 
       title = "Distribution of `hum`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))
```

```{r}
#Retrieve summary statistics for windspeed
windSummary = numSummary(CBS$windspeed, "windspeed")
numVars = bind_rows(numVars, windSummary)
#windSummary

#Windspeed Distribution
wind_plot <- CBS %>% ggplot(aes(windspeed)) +
  geom_histogram(aes(y = after_stat(density)), fill = "blue", color = "black", alpha = 0.6) +
  geom_density(adjust = 3) +
  labs(x = "Normalized Windspeed",
       y = "Density", 
       title = "Distribution of `windspeed`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))
```

```{r}
#Summary Statistics of Continuous Variables 
library(DT)
datatable(numVars, class = 'cell-border stripe')
```

```{r fig.height=4, fig.width=5}
cont_plot <- ggarrange(temp_plot, atemp_plot, wind_plot, hum_plot)
cont_plot
```


##### `holiday`

Inspection of `holiday` revealed dates which were incorrectly labeled. We re-labeled these observations based on official [federally recognized holidays](https://www.commerce.gov/hr/employees/leave/holidays).

|       2011-01-01, 2011-12-25, 2012-01-01, and 2012-11-11 were incorrectly mislabeled as *not* being holidays. 2011-12-26, 2012-01-02, and 2012-11-12 were mislabeled as **being** holidays. 2011-04-15 and 2012-04-16 are dates of observance for Emancipation Day in the Washington D.C. Area. Due to the holiday-like observance of Emancipation Day in our area of interest, these two dates will be labeled as holidays.

```{r}
holidayDates <- CBS[CBS$holiday == TRUE,]
holidayDates$date %>% unique()
```

```{r}
#2011 Holiday Dates
USFed2011 <- c("2011-01-01", "2011-01-17", "2011-02-21","2011-04-15", "2011-05-30", "2011-07-04", "2011-09-05", "2011-10-10", "2011-11-11", "2011-11-24", "2011-12-25")
incHolidayDates2011 <- CBS[CBS$date %in% USFed2011 & CBS$holiday == FALSE,]$date %>% unique()

#2012 Holiday Dates
USFed2012 <- c("2012-01-01", "2012-01-16", "2012-02-20", "2012-04-16", "2012-05-28", "2012-07-04", "2012-09-03", "2012-10-08", "2012-11-11", "2012-11-22", "2012-12-25")
incHolidayDates2012 <- CBS[CBS$date %in% USFed2012 & CBS$holiday == FALSE,]$date %>% unique()

#Merge incorrect dates
incHolidays <- c(incHolidayDates2011, incHolidayDates2012)
print("Should be labeled as holidays: ")
print(incHolidays) 

#Combined Holiday List 
combinedUSFedHolidays <- c(USFed2011, USFed2012)
print("Combined List of Federally Recognized U.S. Holidays for 2011/2012: ")
print(combinedUSFedHolidays)

#Correct Holiday Labels
CBS<- CBS %>% mutate(holiday = ifelse(date %in% c(combinedUSFedHolidays), TRUE, FALSE))
```

```{r fig.width=5}
#Holiday Distribution
hol_plot <- CBS %>% 
  ggplot(mapping = aes(x = holiday, fill = holiday)) +
  geom_bar(color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 1000, size = 3) +
  scale_fill_viridis_d("Year", option = "magma") +
  labs(title = "Distribution of `holiday`",
       x = "",
       y = "") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  guides(fill = "none") +
  coord_flip()
```

```{r}
#Holiday Distribution
work_plot <- CBS %>% 
  ggplot(mapping = aes(x = workingday, fill = workingday)) +
  geom_bar(color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 1000, size = 3) +
  scale_fill_viridis_d("Year", option = "magma") +
  labs(title = "Distribution of `workingday`",
       x = "",
       y = "") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  guides(fill = "none") +
  coord_flip()
```

```{r fig.width=4.2}
bool_plot <- ggarrange(work_plot, hol_plot,  nrow = 2, ncol = 1)
bool_plot
```

## Correlation

|       We investigated the extent to which our variables are linearly related to one another with a correlation matrix~[Appendix-Fig.9]~. Here, we are concerned with relationships between that of our target, `count_rentals` and continuous predictors. Our noteworthy findings include:

-   `temp` and `atemp` both aim to measure normalized Celsius in an objective and subjective way, respectively. It is unsurprising that these variables exhibit a near exact linear relationship with one another. We see they are both equally, positively correlated to our target variable. Naively including both `temp` and `atemp` in our modeling process would result in multicollinearity. To maintain a robust regression model and avoid multicollinearity which would cause our coefficients estimates to become unstable, we will choose only `temp` to be included in the modeling process due to it being an objective measurement as opposed to `atemp` which is subjective.

-   Our target is negatively correlated with `hum`. Being that humidity includes precipitation such as rain, mist, snow, and others, this relationship is unsurprising.

Overall, we see that each of our continuous predictor variables exhibits a mostly straight-line relationship with our target variable~[Appendix-Fig.9 (Bottom Row)]~.

```{r  fig.height=6, fig.width=6}
#Show relationships between predictors and dvs
pred_CBS <- CBS %>% dplyr::select(-c(registered, casual))
cor_matrix <- cor(pred_CBS[sapply(pred_CBS, is.numeric)])

# Visualize the correlation matrix using the corrplot package
corrplot(cor_matrix, method = "circle",
         type = "upper", order = "AOE", 
         tl.col = "black", tl.cex = 1,
         addCoef.col = "darkred",
         number.cex = 1, tl.srt = 50)
```

```{r  fig.height=8, fig.width=8}
library(PerformanceAnalytics)
corr_df <- pred_CBS %>% select_if(is.numeric)
chart.Correlation(corr_df)
```

## Multivariate Analysis

### Temperature and Target Relationship

|       We examined the multi-variable relationship between our temperature related variables, `temp`, `atemp`, and `count_rentals`~[Appendix-Fig.10]~. We revealed twenty-four high-leverage observations having the exact same `atemp` value of 0.2424, but `temp` values which were relatively high and variable in relation, ranging from [0.62 to 0.86]. When considering the observational evidence of these twenty-four `temp` and `atemp` observations as being high leverage outliers, we have opted to remove these observations. This removal helps to ensure data integrity as we move towards the modeling process.

```{r fig.height=4.5, fig.width=5}
temp_mult_plot <- CBS %>% ggplot(aes(atemp, temp)) +
  geom_raster(aes(fill = count_rentals)) +
  labs(x = "Real Feel - Celcius",
       y = "Actual Temp - Celcius",
       title = "Rentals Relationship to Temperature Real & Feel",
       subtitle = "Highest Occurrance During Favorable Degrees of Celcius") +
  scale_fill_viridis_c(option = "magma") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  geom_rect(aes(xmin = .21, ymin = .55, xmax = .27, ymax = .94), color = "red", fill = NA) +
  geom_text(x = .20, y = .5, label = "High Leverage", color = "red")
```

```{r fig.height=4.5, fig.width=5}
#Examine multi-variate outliers to find potential reason of high `temp` values
mvOutliers <- CBS[CBS$atemp < .3 & CBS$temp > .5,]
mvOutliers

#Remove Outlier Observations
subset <- (!(rownames(CBS) %in% rownames(mvOutliers)))
CBS <- CBS[subset, ]

rownames(CBS) <- NULL
```

### Work Commute - Hourly Rental Trends

|       Our intuition led us to investigate possible patterns of usage based solely on time. What we uncovered, showed that during working days, there is strong indication that bicycles are being utilized as a primary mode of transportation to and from work by `registered` riders.  Around 8:00AM, 5:00PM, and 6:00PM, we note high rental demand during the busiest work commute times, with a stark decrease afterwards. Neither `casual` rentals nor rentals occurring on Saturday/Sunday exhibit this trend, adding further evidence to our speculation~[Appendix-Fig.11/12]~. We see that between total rentals and registered rentals, the trend is nearly identical~[Appendix-Fig.13/14]~. 

```{r fig.width=7, fig.height = 4}
#Examining the relationship between day of the week and hourly trends
hours <- CBS$hour %>% unique() %>% as.vector()

#Usage looks identical for casual & registered --> therefore only showing count_rentals
ss_plot <- CBS[(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), count_rentals, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Total Rentals",
       title = "Weekend Hourly Trend of Rentals")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 90,
                                   size = 9,
                                   vjust = .7),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)
```

```{r fig.width=7, fig.height = 8}
#Weekday Trend Over Hours - Casual
wk_cas <- CBS[!(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), casual, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Casual Rentals",
       title = "Weekday Hourly Trend of Casual Rentals")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 90,
                                   size = 9,
                                   vjust = .7),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)
```

```{r fig.width=7, fig.height = 10}
#Weekday Trend Over Hours - Registered
reg_plot <- CBS[!(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), registered, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Registered Rentals",
       title = "Weekday Hourly Trend of Registered Rentals",
       subtitle = "Peak Usage Before & After Work 9-5 Schedule")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 90,
                                   size = 9,
                                   vjust = .7),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)

#Weekday Trend Over Hours - Total
tot_plot <- CBS[!(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), count_rentals, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Total Rentals",
       title = "Weekday Hourly Trend of Total Rentals",
       subtitle = "Peak Usage Before & After Work 9-5 Schedule")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 90,
                                   size = 9,
                                   vjust = .7),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)
```

## Feature Reduction

`date` - Our team reasoned that creation of **730** dummy variables representing `date` could have severe impacts (high VIF and overfitting) on model performance. No unique information is provided by `date` that is not already found in our `year`,  `month`, and `day` variable. Because of this, we have decided to disregard the `date` variable from the modeling process.  

|       Additionally, being that `instant` is merely an identifier, it will also be disregarded. Lastly, being that our target variable is the exact sum of `casual` and `registered`, inclusion of these two would make our predictions arbitrary. We likewise disregarded these from our modeling and assume this information is unknown during predictions.

## Feature Engineering

Dummy Variable Creation - Categorical variables, such as `season`, `year`, `month`, `day`, `hour`, and `weather`, are suited for dummy variable creation for use in our multiple linear regression model. This encoding ensures these variables are appropriately interpreted by our model.

```{r}
#Create Dummy Coding here for these variables 
library(fastDummies)

#Create Dummy Variables
CBS <- dummy_cols(CBS, select_columns = 
                    c("season", "year", "month", "day", "hour", "weather"),
                  remove_first_dummy = T) %>%
  dplyr::select(-c(season, year, month, day, hour, weather))

#Coerce Logical Types to Numeric
CBS <- CBS %>% mutate_if(is.logical, as.numeric)
CBS <- CBS %>% dplyr::select(-c("casual", "registered"))
CBS <- CBS %>% dplyr::select(-c(date, atemp))
```

# Model Building

## Data Partition

|       With our goals centered on finding the most significant predictors with respect to determining the total number of rentals, we decided to partition our data 80% towards training the model and 20% for testing model performance. Allowing a significant majority of our overall dataset to go towards the model fitting process enables us to estimate precise, stable coefficient estimates and better overall model performance.

```{r}
library(leaps) #Best Subset Selection
library(caret) #data split
#For Reproducibility
set.seed(123)
#80/20 Test Split 
index <- createDataPartition(CBS$count_rentals, p = .8, list = FALSE)
train <- CBS[index, ]
test <- CBS[-index, ]
rownames(train) <- 1:nrow(train)
rownames(test) <- 1:nrow(test)
```

## Initial Full Model(s)

|       Our initial, comprehensive full model encompasses all possible predictors within the Capital Bike Share dataset. Our approach allows us to gain a generalized sense as to the importance and effects our predictors are having in determining our response variable. 

```{r, fig.width=6, fig.height=6}
full_model <- lm(count_rentals ~ ., data = train)
summary(full_model)
```


\begin{center}
Initial Full Model 
\end{center}

$\hat{{count rentals}}$ = $\hat{\beta_0}$ + $\hat{\beta_1}$`holiday` + $\hat{\beta_2}$`workingday` + $\hat{\beta_3}$`temp` + ... + $\hat{\beta_{52}}$`weather_Type_4`

Where $\hat{{count rentals}}$ is the predicted number of rentals, $\hat{\beta_0}$ is the estimated intercept, and $\hat{\beta_1}, \hat{\beta_2}, ... \hat{\beta_{52}}$ are the estimated coefficients of the predictor variables~[Appendix-Pages:28/29]~.

|       While the full model containing all possible predictors can provide some insightful information, it is unrefined. Many of the predictors were found to be statistically insignificant in predicting the response. While we could remove these variables, we will allow stepwise selection to determine the best set of predictors by systematically adding and removing variables to determine the best subset of significant predictors. To refine this full model, we will first inspect the assumptions of multiple linear regression.


### Model Diagnostics

#### Normality of Residuals

Our residuals should be normally distributed and can be visualized using a Q-Q plot. Deviations from our straight line in the diagnostic plot would suggest potential non-normality and is confirmed by the formal Kolmogorov-Smirnov Test. We can be certain that our residuals are not normally distributed~[Appendix-Page:30/Fig.15]~.

```{r}
# Kolmogorov-Smirnov Test - Normality Assumption 
ks <- ks.test(full_model$residuals, 'pnorm')
if (ks$p.value <= 0.05) {
  result = paste("H0 rejected: Non-Normal residuals")
} else {
  result = paste("H0 failed to reject: the residuals ARE distributed normally")
}
ks
# Q-Q plot for residuals
qqnorm(resid(full_model))
qqline(resid(full_model))
```

#### Homoscedasticity

The homoscedasticity assumption states that we should have residuals with a constant variance. The funnel-shape we see in our diagnostic plot is indicative of the opposite, heteroscedasticity, or non-constant variance, and is confirmed by our formal Breusch-Pagan Test. Our standard errors, confidence intervals, and subsequently our hypothesis testings rely on the homoscedasticity assumption~[Appendix-Fig.16]~. 


#### Linearity

The linearity assumption states that we should assume the true relationship between our predictors and response variable is a straight-line. We can identify non-linear trends with the red line fit to our residuals. Linearity appears to be violated here, as the upward-curved line is indicative of a non-linear relationship~[Appendix-Fig.16]~. 

```{r fig.height=3.5, fig.width=3.5}
#Breusch-Pagan Test - Homoscedasticity Assumption
library(lmtest)
bp <- bptest(full_model)
  if (bp$p.value <= 0.05) {
    result = paste("H0 rejected: Error variance spread INCONSTANTLY (Heteroscedasticity)")
  } else {
    result = paste("H0 failed to reject: Error variance spread CONSTANTLY (Homoscedasticity)")
  }
bp
print(result)
plot(full_model, which = 1)
```


#### Multicollinearity

The presense of multicollinearity reduces the accuracy in our model's coefficients by causing our coefficients' standard errors to grow, effectively masking their importance and making interpretation difficult. We can detect multicollinearity by calculating the variance inflation factor of our model's predictors. Our first full model contains a few predictors having variance inflation values greater than 10 which warrants possible removal from our model to address multicollinearity~[Appendix-Page:33/34]~.

```{r}
library(car)
df_vif = vif(full_model) %>% data.frame()
colnames(df_vif) = "VIF"
highest_vif <- df_vif %>% arrange(desc(VIF))
highest_vif
```

#### Independence of Errors

Our residuals should be independent of each other for our regression to be reliable. We can see from the Residuals vs. Fitted plot's red line that we have evidence of having violated this assumption. We can confirm that indeed we have a lack independence of errors from the formal Durbin-Watson test. From our diagnostic plot, if our red line were horizontal, this would indicate independence of errors. Our test result shows that indeed there is strong evidence of positive autocorrelation and a lack of independence of errors~[Appendix-Fig.16]~.

```{r fig.height=4, fig.width=4}
library(lmtest)
# Durbin-Watson Test
dwtest(full_model)
```

### Model Improvement

|       We want our final multiple linear regression model to be robust. To achieve this and bring our assumptions closer to be satisfied, we now proceed with various approaches of addressing issues which may be affecting our model's assumptions.

#### Influential Observation Removal

We begin by removing observations which we believe may be having too disproportionate an impact on our model's fit. These observations in particular are likely to disproportionately affect our model's coefficients, affecting our overall predictive accuracy. These include the three observations with significantly high Cook's distances which measures an observation's leverage. Each removed observation were of `weather_type_4`. Because of this, the `weather_type_4` variable was consequently removed from the training data set. Being that there were a mere three observations of `weather_type_4` observed over the entire dataset, we feel justified in removal of this variable and its disproportionate impact on our model outside of normal weather conditions~[Appendix-Fig.17]~. 

```{r}
cd_plot <- plot(full_model, which = 5)
```

```{r fig.height=12, fig.width=12}
#Rownames were reset during train/test split
out_lev <- c(450, 7302, 7093)
train <- train[-c(out_lev), ]
train <- train %>% dplyr::select(-`weather_Type 4`)
```

#### Target Variable Transformation

We considered multiple transformations to approach normality and address the linearity and homoscedasticity assumptions, including, the Box-cox, logarithmic, square-root, cube-root, and fourth-root transformations. The transformation which approximated normality (skewness value closest to 0) the closest was the **cube-root** transformation. Therefore, we will predict the cube-root transformed version of our target variable `cube_root_total` for the final model~[Appendix-Page:36/37]~.

```{r}
library(MASS)
#Box-cox Transformation on count_rentals
b <- boxcox(lm(CBS$count_rentals ~ 1), plotit = F, lambda = seq(-3,3, .05))
lambda <- b$x[which.max(b$y)]
box_total <- (CBS$count_rentals ^ lambda - 1) / lambda
#Log Transformation
log_total <- log(CBS$count_rentals)
#Sqrt Transformation
sqrt_total <- sqrt(CBS$count_rentals)
#Cube Root Transformation
cube_root_total <- (CBS$count_rentals)^(1/3)
#Fourth Root Transformation
fourth_root_total <- (CBS$count_rentals)^(1/4)

ggTrans <- function(var, myTitle, my_axis) {
  #Cube Root Transformation - count_rentals
  skewVal <- skewness(var) %>% round(3)
  CBS %>% ggplot(aes(var)) +
    geom_histogram(aes(y = after_stat(density)),fill = "darkblue", color = "black", alpha = 0.6) +
    geom_density(adjust = 2) +
    labs(x = paste(my_axis), y = "Density",
         title = myTitle,
         subtitle = paste("Skewness Value: ", skewVal)) +
    theme(
      plot.title = element_text(hjust = 0, face = "bold", size = 12),
      axis.title.x = element_text(size = 8, color = "black"),
      axis.title.y = element_text(size = 8, color = "black"),
      axis.text.x = element_text(size = 8, color = "black"),
      axis.text.y = element_text(size = 8, color = "black"))
}
```

```{r fig.height=8, fig.width=3}
#Inspect Distributions
pl1 <- ggTrans(cube_root_total, "Cube Root Transformation", "Cube Root Total Rentals")
pl2 <- ggTrans(box_total, "Box-Cox Transformation", "Box-cox Total Rentals")
pl3 <- ggTrans(fourth_root_total, "Fourth Root Transformation", "Fourth Root Total Rentals")
pl4 <- ggTrans(sqrt_total, "Square Root Transformation", "Square Root Total Rentals")
pl5 <- ggTrans(log_total, "Log Transformation", "Log Total Rentals")
trans_total <- ggarrange(pl1, pl2, pl3, pl4, pl5, nrow = 5, ncol = 1)
trans_total
```

```{r}
#Add cube-root transformations to the train dataset
train$cube_root_total <- (train$count_rentals)^(1/3)
```

#### Multicollinearity

We opted to remove the influences of multicollinearity in our models by removing the variables which had the highest variance inflation factors(VIF > 10) one at a time. The variables which were removed were `workingday`, and `season_Summer`.

```{r, fig.width=7, fig.height=7}
#Could remove workingday & season_Summer from train df
train <- train %>% dplyr::select(-c( workingday, season_Summer))
```

## Final Reduced Model Using Stepwise Selection

|       Using stepwise selection, we identified an optimal subset of predictors. These variables were determined to be the best set of predictors in predicting the transformed response, `cube_root_total`~[Appendix-Page:38/39]~. The final stepwise model formula can be found in the Appendix on Page 40.

```{r fig.height=5, fig.width=8.5}
library(leaps)
OLS.regback <- leaps::regsubsets(cube_root_total ~ . - count_rentals, train, method = "seqrep", nvmax = 45)
OLSregbacksum <- summary(OLS.regback)
```

|       Our team believes that two of the best metrics for determining model fit are Adjusted $R^2$ and Bayesian Information Criterion (BIC) among others. Both of these metrics help determine models which balance goodness-of-fit and complexity. We seek to maximize Adjusted $R^2$ and minimize BIC through stepwise selection's repeated addition and removal of predictors to determine best fit~[Appendix-Fig.19]~.

```{r fig.height=8, fig.width=8.5}
adjr2mat <- data.frame(t(OLSregbacksum$adjr2))
bicmat <- data.frame(t(OLSregbacksum$bic))
cpmat <- data.frame(t(OLSregbacksum$cp))
rsqmat <- data.frame(t(OLSregbacksum$rsq))

combined_df <- data.frame(
  R2 = unlist(rsqmat),
  Adj_R2 = unlist(adjr2mat),
  BIC = unlist(bicmat),
  Mallow_Cp = unlist(cpmat)
)

adjr2 <- combined_df[36,]$Adj_R2 %>% round(2)
bic <- combined_df[36,]$BIC %>% round(2)
cp <- combined_df[36,]$Mallow_Cp %>% round(2)

par(mfrow = c(2,2))
plot(OLSregbacksum$adjr2, xlab = "Number of Variables",
    ylab = "Adjusted RSq", type = "l", lwd = 1.5, main = "Number of Variables Vs. Adjusted R2",
    sub = paste("Best Adjusted RSq Value: " , adjr2 ),
    cex.main = 1.15, cex.lab = 1, cex.axis = 1.05, font.axis = 2,
    font.lab = 2, panel.first = grid(nx = NULL, ny = NULL, col = "gray",
        lty = 2))
points(37, OLSregbacksum$adjr2[37], col = "darkred", cex = 2, pch = 20) 

plot(OLSregbacksum$cp, xlab = "Number of Variables", ylab = "Cp",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. Mallow's Cp",
    sub = paste("Best Mallow's Cp Value: " , cp ),
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
points(37, OLSregbacksum$cp[37], col = "darkred", cex = 2, pch = 20)

plot(OLSregbacksum$bic, xlab = "Number of Variables", ylab = "BIC",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. BIC",
            sub = paste("Best BIC Value: " , bic ),
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
points(37, OLSregbacksum$bic[37], col = "darkred", cex = 2, pch = 20)

plot(OLSregbacksum$rss, xlab = "Number of Variables", ylab = "RSS",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. RSS",
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
```

```{r}
# Find the model with the lowest BIC value
best_model_index <- which.min(OLSregbacksum$bic)

# Get the best model
best_model <- coef(OLS.regback, id = best_model_index)

# Extract the coefficients from the best model and remove the intercept term from the coefficients
best_coefficients <- round(unname(best_model)[-1], 4)

# Get the variable names from your redDF dataframe
variable_names <- names(best_model)[-1]

# Extract the intercept coefficient separately
intercept_coefficient <- round(unname(best_model)[1], 4)

# Define the named_coefficients variable
named_coefficients <- setNames(best_coefficients, variable_names)

# Create the LaTeX equation
equation <- paste("y =", intercept_coefficient, "+", paste(named_coefficients, variable_names, sep = " * ", collapse = " + "), "")
```

```{r}
#---Prepare Final Dataset---  
#Extract the best stepwise variables from train
final_mod_vars <- named_coefficients %>% names()
vars <- train %>% names()

#Fix "`" and then return list of appropriate vars
variables_for_selection = c()
for (i in 1:length(final_mod_vars)) {
  var_to_list = str_replace_all(final_mod_vars[i], "`", "" )
  variables_for_selection = c(variables_for_selection, var_to_list)
}

#Prepare Final Training Dataset Based on Selected Variables
train_final <- train %>% select_if(vars %in% variables_for_selection)
train_final <- train_final %>% bind_cols(train$cube_root_total)
colname = train_final %>% names()
train_final <- train_final %>% rename(cube_root_total = colname[length(colname)])
```

```{r}
#Train Final Model - Success (Same coef as original)
stepwise_model <- lm(cube_root_total ~ .  ,data = train_final)
summary(stepwise_model)
```

### Interpretation of Model Coefficients

|       From our step-wise model, the three variables with the most significant impact on the expected number of total rentals are `hour_5:00PM`, `hour_6:00PM`, and `hour_8:00AM`.

|       For instance, when our `hour_5:00PM` variable is 1(True), holding other variables constant, the cube root number of bicycle rentals is expected to increase by 4.23614. In other words, if we back-transform our coefficient values for interpretability, the number of bicycle rentals is expected to increase by approximately 77 total bicycle rentals when it is 5:00pm. This procedure of coefficient interpretation through back-transformation applies to all model coefficients. In the case of continuous variables, such as `temp`, the expected increase in the number of bicycle rentals is determined by a one unit increase in the predictor value.

```{r fig.height=7, fig.width=6}
library(coefplot)
coefplot::coefplot(stepwise_model, sort = "magnitude", decreasing = TRUE) 
```

### Model Diagnostics

|       Our assumptions mentioned after the first full model fitting were promptly addressed with our transformation of the response variable via a cube root transformation, and removal of over-influential observations and high variance inflation variables. Despite our approach, we did observe some potential violations of regression assumptions in homoscedasticity, linearity, and normality/independence of residuals. The multicollinearity assumption was satisfied due to our removing of high variance inflation variables~[Appendix-Pages:41/42-Fig.18]~.  

|       In practice, however, the assumptions are rarely validated. After employing our corrective measures and observing the vastly improved diagnostic plots, we remain confident in our model's predictive capabilities and generalizability. Our attempts at rectifying our assumptions helped to stabilize our coefficients, and ensure that our model contains meaningful variables in determining our target~[Appendix-Fig.20]~.

```{r fig.height=5, fig.width=5}
#Linearity-Homoscedasticity
plot(stepwise_model, which = 1, main = "Evaluation of Homoscedasticity and Linearity")

# Kolmogorov-Smirnov Test - Normality Assumption 
ks <- ks.test(stepwise_model$residuals, 'pnorm')
if (ks$p.value <= 0.05) {
  result = paste("H0 rejected: Non-Normal residuals")
} else {
  result = paste("H0 failed to reject: the residuals ARE distributed normally")
}
ks

# Q-Q plot for residuals (Normality of Residuals)
qqnorm(resid(stepwise_model))
qqline(resid(stepwise_model))

# Durbin-Watson Test (Independence of Residuals)
dwtest(stepwise_model)

#Multicollinearity
vif_final <- vif(stepwise_model) %>% data.frame()
vif_final <- vif_final %>% rename(Highest_VIF_Values = ".") %>% arrange(desc(Highest_VIF_Values))
vif_final %>% head(5)
```

### Test Set Evaluation - Model Performance  

|       Utilizing the final step-wise model determined above, we tested our model's performance by making predictions on the test set. Because we applied a cube-root transformation to our target variable `cube_root_total`, we back-transform these predicted values for interpretability. Doing so allows us to draw meaningful interpretation from our model's performance in the original units as the response.  

|       Here we discuss the test performance metrics of Root Mean Squared Error(RMSE), $R^2$, and Mean Absolute Error(MAE). RMSE and MAE are measured in the same units as the original target variable and thus have a meaningful interpretation.  MAE for example, is the "on-average" error between the predicted value and the true number of bike rentals, which for our model is approximately **63** rentals. Additionally, our $R^2$ value is approximately 0.733, meaning that 73.3% of the variability in the number of bike rentals is explained by our model. Our RMSE is approximately 95.5, which is indicative of strong, predictive accuracy in our model~[Appendix-Pg:48]~.

|       From our visualization displaying predictions versus true observational values, we see that our residuals are fairly small when the number of actual rentals are low. The residuals slightly increase in magnitude as we make predictions for larger numbers of rentals which inflates our performance metrics~[Appendix-Fig.21]~. Overall, we believe this model to be of high quality and generalizable to new data.  


```{r}
#Make Predictions
pred_values_total_rentals <- predict(stepwise_model, newdata = test) %>% data.frame()
pred_values_total_rentals$fit = (pred_values_total_rentals$.)^3
pred_values_total_rentals$obs = test$count_rentals


#Return Metrics
metrics <- function(y_pred, y_true){
 rmse <- RMSE(y_pred, y_true) %>% round(2)
 mse <- mean((y_pred - y_true)^2) %>% round(2)
 mae <- MAE(y_pred, y_true) %>% round(2)
 corPredAct <- cor(y_pred, y_true) %>% round(3)
 df = data.frame(Root_Mean_Squared_Error = rmse,
                 Mean_Absolute_Error = mae,
                 R_squared_Pred_Obs = (corPredAct*corPredAct) %>% round(3))
 df <- df %>% t() %>% data.frame()
 df <- df %>% rename(Metric = ".")
 return (df)
}
model_metrics_df = metrics(y_pred = pred_values_total_rentals$fit,
                           y_true = pred_values_total_rentals$obs)
model_metrics_df
```

```{r fig.height=5, fig.width=7}
#Total Rentals Predicts vs. Observed Values
a_v_p_plot <- ggplot(pred_values_total_rentals, aes(x=fit, y=obs)) +
  geom_point(aes(color = abs(obs - fit)), size = 3, alpha = 0.6) + 
  geom_abline(intercept=0, slope=1, linetype="dashed", color="red") +
  scale_color_viridis_c(option = "B", direction = -1) + 
  labs(
    title = "Actual vs Predicted Number of Total Rentals",
    subtitle = paste("Average Prediction Error(Mean Absolute Error): ", model_metrics_df$Metric[2], "bike rentals"),
    y = "Actual Number of Rentals",
    x = "Predicted Number of Rentals",
    color = "Residual Magnitude") +
  scale_x_continuous(breaks = seq(0, 1000, 250), labels = seq(0,1000,250)) +
   scale_y_continuous(breaks = seq(0, 1000, 250), labels = seq(0,1000,250)) +
  theme(legend.position = "bottom",
        axis.text= element_text(face = "bold",
                                   size = 12),
        plot.title = element_text(face = "bold",
                                  size = 16))
a_v_p_plot
```

# Conclusions

|       The Capital Bike Share dataset analysis revealed insightful patterns and statistics in pursuit of variables which were used in formulating a reliable multiple linear regression model. Significant predictors in determining the total number of bicycle rentals found when examining our data, such as the hour of the day and temperature, help us understand which features highly influence the number of total rentals.

|       Our final model provides a concise selection of key predictors, allowing us to filter out non-meaningful features which would otherwise prove irrelevant. With our final multiple linear regression consisting of only the most paramount variables in determining the number of rentals, Capital Bike Share can feel confident in making accurate and reliable predictions.

## Recommendations

|       Based on the hourly working day commute trends showing commutes to and from the workplace as being a major driving force in the number of rentals, and the extreme prevalence and significance of the hour variables present in our final model, we will lastly provide actionable insight for Capital Bike Share. 

|       We are certain that registered commuters are utilizing bicycles as their primary mode of transportation during workplace commutes. Capital Bike Share can act on this, by installing/placing additional bicycle kiosks in key residential areas and near workplaces with traffic-heavy commute roads. Doing so would incentivize additional bike rentals by providing commuters a reliable source of workplace transportation, ultimately increasing the number of bicycle rentals. Additionally, to improve future modeling, we recommend gathering more observations with larger numbers of rentals, as this data would improve predictive accuracy at those larger values of rentals. A lack of high-rental observations is more than likely the cause of our large residuals when predicting high numbers of rentals.

\pagebreak

# Appendix

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Visualize for Missing Data
vis_miss(CBS, sort_miss = T) + 
  labs(y = "NA Values") +
  theme(axis.text.x.bottom = element_text(angle = 60, vjust = 0)) +
  coord_flip()
```

```{r echo=FALSE, fig.align='center', fig.cap="No missing values in dataset", fig.height=3, fig.show='hold', fig.width=6, include=TRUE}
fig.1
```

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Visualize the Pre-assigned Data Types
vis_dat(CBS) + theme(axis.title.y = element_blank(),
                     axis.text.x.bottom = element_blank()) +
  labs(title = "Data Types of Capital Bike Share Dataset") +
  scale_fill_brewer(palette = 2)
``` 

```{r echo=FALSE, fig.align='center', fig.cap="Pre-assigned Variable Data Types", fig.height=2.2, fig.show='hold', fig.width=6, include=TRUE}
fig.2
```

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Visualize the Reassigned Data Types
vis_dat(CBS) + theme(axis.title.y = element_blank(),
                     axis.text.x.bottom = element_blank()) + 
  scale_fill_brewer(palette = 2)
```

```{r echo=FALSE, fig.align='center', fig.cap="Reassigned Variable Data Types", fig.height=2.7, fig.show='hold', fig.width=6, include=TRUE}
datatype_plot
```

\pagebreak 

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Show the distribution of the target variable, `count_rentals`
CBS %>% ggplot(aes(count_rentals)) +
  geom_histogram(aes(y = after_stat(density)),fill = "darkblue", color = "black", alpha = 0.6) +
  geom_density(adjust = 2) +
  labs(x = "Total Number of Rentals", y = "Density",
       title = "Distribution of `count_rentals`",
       subtitle = paste("Skewness Value: ", results$Skew),
       caption = "Median Marked By Dashed Red Line") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")
  ) +
  geom_vline(aes(xintercept = results$Median), color = "red", linetype = "dashed")
```


```{r echo=FALSE, fig.align='center', fig.cap="Distribution of Target Variable", fig.show='hold', fig.width=5, include=TRUE}
target_plot
```


\pagebreak  

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Continuous variables distributions (hum, atemp, temp, windspeed) were all generated utilizing the same structure of ggplot seen below. `temp` is show as a general example. 
CBS %>% ggplot(aes(temp)) +
  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = "blue", color = "black", alpha = 0.6) +
  geom_density(adjust = 3) +
  labs(x = "Normalized Celcius",
       y = "Density", 
       title = "Distribution of `temp`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))

cont_plot <- ggarrange(temp_plot, atemp_plot, wind_plot, hum_plot)
```


```{r fig.height=4, fig.width=4.5, echo=FALSE, fig.align='center', fig.cap="Continuous Variables Distribution", fig.show='hold', include=TRUE}
cont_plot
```

\pagebreak

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Summary Statistics of Continuous Variables were generated utilizing self-made function then row-binded into the datatable seen below
numSummary = function(var, rowname) {
  tab = data.frame(Mean = mean(var) %>% round(2),
           Median = median(var),
           StdErr = sd(var) %>% round(2),
           Skew = skewness(var) %>% round(2),
           Q1 = quantile(var, .25),
           Q3 = quantile(var, .75),
           IQR = IQR(var),
           Min = min(var),
           Max = max(var))
  rownames(tab) = rowname
  return(tab)
}
```


```{r fig.height=4, fig.width=8.5, echo=TRUE, fig.align='center', fig.cap="Continuous Variables Summary Statistics", fig.show='hold', include=TRUE}
#Visualize our Continuous Variables Summary Statistics
datatable(numVars, class = 'cell-border stripe', options = list(searching = FALSE,
                                                                dom = 't'))
```

\pagebreak

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Boolean variables distributions (workingday, holiday) were all generated utilizing the same structure of ggplot seen below. `workingday` is show as a general example. 
CBS %>% 
  ggplot(mapping = aes(x = workingday, fill = workingday)) +
  geom_bar(color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 1000, size = 3) +
  scale_fill_viridis_d("Year", option = "magma") +
  labs(title = "Distribution of `workingday`",
       x = "",
       y = "") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  guides(fill = "none") +
  coord_flip()

#Organize all boolean variables for plotting
bool_plot <- ggarrange(work_plot, hol_plot,  nrow = 2, ncol = 1)
bool_plot
```

```{r fig.width=4.2, include = TRUE}
bool_plot
```

\pagebreak

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Continuous variables distributions (hour, month, season, year, day, weather) were all generated utilizing the same structure of ggplot seen below. `month` is shown as an example
CBS %>% 
  group_by(month) %>%
  mutate(countMonth = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((month), desc(countMonth)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(month), desc(countMonth)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 75, size = 3) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `month`",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
```


```{r fig.height=4, fig.width=8,  echo=FALSE, fig.align='center', fig.cap="Distributions of `season`, `year`, `month`, and `day`", fig.show='hold', include=TRUE}
cat_plot_1 <- ggarrange(season_plot, year_plot, month_plot, day_plot)
cat_plot_1
```

```{r fig.height=5, fig.width=7.5, echo=FALSE, fig.align='center', fig.cap="Distributions of `hour`, and `weather`", fig.show='hold', include=TRUE}
cat_plot_2 <- ggarrange(hour_plot, weather_plot, nrow = 2, ncol = 1)
cat_plot_2
```

\pagebreak  

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Observe Low Frequency of Particular Dates
low_count_obs <- CBS %>% group_by(date) %>%
  summarise(count_hour_obs = n()) %>%
  filter(count_hour_obs < 23)

#Expected - Actual
lost_hours = (14*24) - sum(low_count_obs$count_hour_obs)

#Display Date Distribution with only those dates with fewer counts of hourly observations
CBS %>% group_by(date) %>% 
  mutate(dateCount = n()) %>% 
  filter(dateCount < 23) %>%
  ggplot(mapping = aes(x = fct_reorder((as.character(date)), desc(dateCount)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(date), desc(dateCount)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 2, size = 4.4) +
  scale_fill_viridis_d("Date", option = "magma") +
  guides(fill = "none") +
  labs(x = "Date",
       y = "Number of Observations",
       title = "Dates with Missing Hourly Observations") +
  theme(axis.text.x = element_text(angle = 90),
        plot.caption = element_text(size = 8))
```

```{r fig.height=3, fig.width=5,  fig.align='center', fig.cap="Dates with Low Number of Observations",  include=TRUE}
date_plot
```

\pagebreak 

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Show relationships and correlations between continuous predictors and target variable with a Correlation Matrix
pred_CBS <- CBS %>% dplyr::select(-c(registered, casual))
library(PerformanceAnalytics)
corr_df <- pred_CBS %>% select_if(is.numeric)
chart.Correlation(corr_df)
```


```{r fig.height=6, fig.width=6,  fig.align='center', fig.cap="Correlation Matrix and Continuous Variables Relationship with Target Variable", echo = FALSE, include=TRUE}
corr_mat <- chart.Correlation(corr_df)
corr_mat
```

\pagebreak

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Outliers within temp/atemp geom_raster plot
CBS %>% ggplot(aes(atemp, temp)) +
  geom_raster(aes(fill = count_rentals)) +
  labs(x = "Real Feel - Celcius",
       y = "Actual Temp - Celcius",
       title = "Rentals Relationship to Temperature Real & Feel",
       subtitle = "Highest Occurrance During Favorable Degrees of Celcius") +
  scale_fill_viridis_c(option = "magma") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  geom_rect(aes(xmin = .21, ymin = .55, xmax = .27, ymax = .94), color = "red", fill = NA) +
  geom_text(x = .20, y = .5, label = "Outliers", color = "red")
```

```{r fig.height=4, fig.width=5,  fig.align='center', fig.cap="High Leverage Points Between `temp` and `atemp`",  include=TRUE}
temp_mult_plot
```

\pagebreak

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Examining the relationship between day of the week and hourly trends
hours <- CBS$hour %>% unique() %>% as.vector()

#Usage looks identical for casual & registered --> therefore only showing count_rentals
CBS[(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), count_rentals, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Total Rentals",
       title = "Weekend Hourly Trend of Rentals")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 90,
                                   size = 9,
                                   vjust = .7),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)
```

```{r fig.height=4, fig.width=5,  fig.align='center', fig.cap="Smooth Curve of Rental with Peak Usage Around Midday",  include=TRUE}
ss_plot
```

\pagebreak  

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Weekday Trend Over Hours - Casual
CBS[!(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), casual, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Casual Rentals",
       title = "Weekday Hourly Trend of Casual Rentals")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 90,
                                   size = 9,
                                   vjust = .7),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)
```

```{r fig.height=8, fig.width=6,  fig.align='center', fig.cap="Low Casual Usage Throughout Working Days",  include=TRUE}
wk_cas
```

\pagebreak  

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Weekday Trend Over Hours - Registered
CBS[!(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), registered, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Registered Rentals",
       title = "Weekday Hourly Trend of Registered Rentals",
       subtitle = "Peak Usage Before & After Work 9-5 Schedule")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 90,
                                   size = 9,
                                   vjust = .7),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)
```


```{r fig.height=8, fig.width=6,  fig.align='center', fig.cap="Registered Work Commute Usage",include=TRUE}
reg_plot
```

\pagebreak   

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Weekday Trend Over Hours - Total
CBS[!(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), count_rentals, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Total Rentals",
       title = "Weekday Hourly Trend of Total Rentals",
       subtitle = "Peak Usage Before & After Work 9-5 Schedule")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 90,
                                   size = 9,
                                   vjust = .7),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)
```


```{r fig.height=8, fig.width=6,  fig.align='center', fig.cap="Total Rentals Work Commute Usage", include=TRUE}
tot_plot
```

\pagebreak  



\begin{center}
Initial Full Model
\end{center}
 
--- 

```{r include = TRUE, echo = TRUE}
summary(full_model)
```

\pagebreak 

```{r fig.height=5, fig.width=5,  fig.align='center', fig.cap="Normality of Residuals Assumption - Violated", echo = TRUE, include=TRUE}
# Kolmogorov-Smirnov Test - Normality Assumption 
ks.test(full_model$residuals, 'pnorm')
if (ks$p.value <= 0.05) {
  print("H0 rejected: Residuals are NOT normally distributed")
} else {
  print("H0 failed to reject: the residuals ARE distributed normally")
}

# Q-Q plot for residuals
qqnorm(resid(full_model))
qqline(resid(full_model))
``` 

\pagebreak  

```{r fig.height=5, fig.width=5,  fig.align='center', fig.cap="Homoscedasticity/Linearity/Independence of Residuals Assumptions - Violated", echo = TRUE, include=TRUE}
#Breusch-Pagan Test - Homoscedasticity Assumption
bp <- bptest(full_model)
  if (bp$p.value <= 0.05) {
    print("H0 rejected: Error variance spread INCONSTANTLY (Heteroscedasticity)")
  } else {
    print("H0 failed to reject: Error variance spread CONSTANTLY (Homoscedasticity)")
  }


plot(full_model, which = 1)
```

\pagebreak 

```{r fig.cap="No Multicollinearity Assumption - Violated", echo = TRUE, include=TRUE}
print.noquote("Below: Variance Inflation Factors of Full Model")
library(car)
df_vif = vif(full_model) %>% data.frame()
colnames(df_vif) = "VIF"
highest_vif <- df_vif %>% arrange(desc(VIF))
highest_vif
print.noquote("---")
``` 

\pagebreak 

```{r fig.height=5, fig.width=5,  fig.align='center', fig.cap="High Influential Observations", echo = TRUE, include=TRUE}
plot(full_model, which = 5)
```

\pagebreak 

```{r include = TRUE, echo = TRUE, eval = FALSE} 
library(MASS)
#Box-cox Transformation on count_rentals
b <- boxcox(lm(CBS$count_rentals ~ 1), plotit = F, lambda = seq(-3,3, .05))
lambda <- b$x[which.max(b$y)]
box_total <- (CBS$count_rentals ^ lambda - 1) / lambda
#Log Transformation
log_total <- log(CBS$count_rentals)
#Sqrt Transformation
sqrt_total <- sqrt(CBS$count_rentals)
#Cube Root Transformation
cube_root_total <- (CBS$count_rentals)^(1/3)
#Fourth Root Transformation
fourth_root_total <- (CBS$count_rentals)^(1/4)

ggTrans <- function(var, myTitle, my_axis) {
  #Cube Root Transformation - count_rentals
  skewVal <- skewness(var) %>% round(3)
  CBS %>% ggplot(aes(var)) +
    geom_histogram(aes(y = after_stat(density)),fill = "darkblue", color = "black", alpha = 0.6) +
    geom_density(adjust = 2) +
    labs(x = paste(my_axis), y = "Density",
         title = myTitle,
         subtitle = paste("Skewness Value: ", skewVal)) +
    theme(
      plot.title = element_text(hjust = 0, face = "bold", size = 12),
      axis.title.x = element_text(size = 8, color = "black"),
      axis.title.y = element_text(size = 8, color = "black"),
      axis.text.x = element_text(size = 8, color = "black"),
      axis.text.y = element_text(size = 8, color = "black"))
}

#Inspect Distributions
pl1 <- ggTrans(cube_root_total, "Cube Root Transformation", "Cube Root Total Rentals")
pl2 <- ggTrans(box_total, "Box-Cox Transformation", "Box-cox Total Rentals")
pl3 <- ggTrans(fourth_root_total, "Fourth Root Transformation", "Fourth Root Total Rentals")
pl4 <- ggTrans(sqrt_total, "Square Root Transformation", "Square Root Total Rentals")
pl5 <- ggTrans(log_total, "Log Transformation", "Log Total Rentals")
ggarrange(pl1, pl2, pl3, pl4, pl5, nrow = 5, ncol = 1)
```

```{r fig.height=8, fig.width=3, include = TRUE, echo = FALSE}
trans_total
```

\pagebreak  

\begin{center}
Final Model Summary with $\hat{\beta}$ Coefficient Estimates
\end{center}

```{r echo = TRUE, include = TRUE}
summary(stepwise_model)
```


\pagebreak

\begin{center}
Final Model Equation Predicting Cube Root of Rentals
\end{center}

\begin{center}
Corresponding $\hat{\beta}$ Coefficients Above(Model Summary)
\end{center}

--- 

$\hat{cuberoottotal}$ = $\hat{\beta_0}$ + $\hat{\beta_1}$`holiday` + $\hat{\beta_2}$`temp` + $\hat{\beta_3}$`hum` + $\hat{\beta_4}$`season_Spring` + $\hat{\beta_5}$`season_Fall` + $\hat{\beta_6}$`year_2012` + $\hat{\beta_7}$`month_May` + $\hat{\beta_8}$`month_Aug` + $\hat{\beta_9}$`month_Sep` +$\hat{\beta_{10}}$`day_Friday` + $\hat{\beta_{11}}$`day_Saturday` + $\hat{\beta_{12}}$`hour_1:00PM` + $\hat{\beta_{13}}$`hour_10:00AM` + $\hat{\beta_{14}}$`hour_10:00PM` + $\hat{\beta_{15}}$`hour_11:00AM` + $\hat{\beta_{16}}$`hour_11:00PM` + $\hat{\beta_{17}}$`hour_12:00AM` + $\hat{\beta_{18}}$`hour_12:00PM` + $\hat{\beta_{19}}$`hour_2:00AM` + $\hat{\beta_{20}}$`hour_2:00PM` + $\hat{\beta_{21}}$`hour_3:00AM` + $\hat{\beta_{22}}$`hour_3:00PM` + $\hat{\beta_{23}}$`hour_4:00AM` + $\hat{\beta_{24}}$`hour_4:00PM` + $\hat{\beta_{25}}$`hour_5:00AM` + $\hat{\beta_{26}}$`hour_5:00PM` + $\hat{\beta_{27}}$`hour_6:00AM` + $\hat{\beta_{28}}$`hour_6:00PM` + $\hat{\beta_{29}}$`hour_7:00AM` + $\hat{\beta_{30}}$`hour_7:00PM` + $\hat{\beta_{31}}$`hour_8:00AM` + $\hat{\beta_{32}}$`hour_8:00PM` + $\hat{\beta_{33}}$`hour_9:00AM` + $\hat{\beta_{34}}$`hour_9:00PM` + $\hat{\beta_{35}}$`weather_Type_2` + $\hat{\beta_{36}}$`weather_Type_3`

---  

\pagebreak

```{r fig.height=8, fig.width=4,  fig.align='center', fig.cap="Assumption Corrections and High Leverage Removal", echo = TRUE, include=TRUE}
print.noquote("Below: Variance Inflation Factors of Final Model")
vif(stepwise_model)
print.noquote("---")

ks.test(stepwise_model$residuals, 'pnorm')
bptest(stepwise_model)

# Q-Q plot for residuals
par(mfrow = c(3,1))
qqnorm(resid(stepwise_model))
qqline(resid(stepwise_model))
plot(stepwise_model, which = 1)
plot(stepwise_model, which = 5)
```

\pagebreak

```{r fig.height=10, fig.width=9,  fig.align='center', fig.cap="Stepwise Selection Model Evaluations(BIC/RSquared/Mallows Cp/RSS)", include=TRUE, echo  = TRUE}
#Visualize Stepwise Selection Metrics
par(mfrow = c(2,2))
plot(OLSregbacksum$adjr2, xlab = "Number of Variables",
    ylab = "Adjusted RSq", type = "l", lwd = 1.5, main = "Number of Variables Vs. Adjusted R2",
    sub = paste("Best Adjusted RSq Value: " , adjr2 ),
    cex.main = 1.15, cex.lab = 1, cex.axis = 1.05, font.axis = 2,
    font.lab = 2, panel.first = grid(nx = NULL, ny = NULL, col = "gray",
        lty = 2))
points(36, OLSregbacksum$adjr2[36], col = "darkred", cex = 2, pch = 20) 

plot(OLSregbacksum$cp, xlab = "Number of Variables", ylab = "Cp",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. Mallow's Cp",
    sub = paste("Best Mallow's Cp Value: " , cp ),
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
points(36, OLSregbacksum$cp[36], col = "darkred", cex = 2, pch = 20)

plot(OLSregbacksum$bic, xlab = "Number of Variables", ylab = "BIC",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. BIC",
            sub = paste("Best BIC Value: " , bic ),
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
points(36, OLSregbacksum$bic[36], col = "darkred", cex = 2, pch = 20)

plot(OLSregbacksum$rss, xlab = "Number of Variables", ylab = "RSS",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. RSS",
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
``` 

\pagebreak

```{r fig.height=8.5, fig.width=5.5,  fig.align='center', fig.cap="Significant Predictors for Final Model", include=TRUE, echo = TRUE}
coefplot::coefplot(stepwise_model, sort = "magnitude", decreasing = TRUE) + labs(title = "Final Stepwise Model Variable Coefficients", x = "Coefficient Value", y = "Predictor") 
``` 

\pagebreak   

```{r include = TRUE, echo = TRUE, eval = TRUE}
#Make Predictions
pred_values_total_rentals <- predict(stepwise_model, newdata = test) %>% data.frame()
pred_values_total_rentals$fit = (pred_values_total_rentals$.)^3
pred_values_total_rentals$obs = test$count_rentals


#Return Metrics
metrics <- function(y_pred, y_true){
 rmse <- RMSE(y_pred, y_true) %>% round(2)
 mse <- mean((y_pred - y_true)^2) %>% round(2)
 mae <- MAE(y_pred, y_true) %>% round(2)
 corPredAct <- cor(y_pred, y_true) %>% round(3)
 #Only concerned with RMSE, MAE, and R2
 df = data.frame(Root_Mean_Squared_Error = rmse,
                 Mean_Absolute_Error = mae,
                 R_squared_Pred_Obs = (corPredAct*corPredAct) %>% round(3))
 df <- df %>% t() %>% data.frame()
 df <- df %>% rename(Metric = ".")
 return (df)
}

model_metrics_df = metrics(y_pred = pred_values_total_rentals$fit,
                           y_true = pred_values_total_rentals$obs)



model_metrics_df
```

\pagebreak

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Predicted vs. Observed Scatterplot w/ Magnitude of Residuals
ggplot(pred_values_total_rentals, aes(x=fit, y=obs)) +
  geom_point(aes(color = abs(obs - fit)), size = 3, alpha = 0.6) + 
  geom_abline(intercept=0, slope=1, linetype="dashed", color="red") +
  scale_color_viridis_c(option = "B", direction = -1) + 
  labs(
    title = "Actual vs Predicted Number of Total Rentals",
    subtitle = paste("Average Prediction Error(Mean Absolute Error): ", model_metrics_df$Metric[2], "bike rentals"),
    y = "Actual Number of Rentals",
    x = "Predicted Number of Rentals",
    color = "Residual Magnitude") +
  scale_x_continuous(breaks = seq(0, 1000, 250), labels = seq(0,1000,250)) +
   scale_y_continuous(breaks = seq(0, 1000, 250), labels = seq(0,1000,250)) +
  theme(legend.position = "bottom",
        axis.text= element_text(face = "bold",
                                   size = 12),
        plot.title = element_text(face = "bold",
                                  size = 16))
```

```{r fig.height=9, fig.width=9,  fig.align='center', fig.cap="Predictions Using the Test Dataset", include=TRUE}
a_v_p_plot
```

\pagebreak  



