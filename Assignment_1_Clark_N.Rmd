---
title: "Capital Bike Share Predictive Model Report"
author: "Prepared By: Clark P. Necciai Jr."
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    number_sections: yes
    toc_depth: '3'
fontsize: 12pt
geometry: margin=.80in
header-includes:
- \usepackage{setspace}
- \singlespacing
linkcolor: blue
---

```{r setup, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
#Default Setup
knitr::opts_chunk$set(include = FALSE, echo = FALSE, tidy=TRUE, cache=FALSE, message=FALSE, warning=FALSE,tidy.opts=list(width.cutoff=55, keep.blank.line=FALSE),fig.align='center', fig.width=3.50, fig.height=2.75, dpi = 300)
```  

\pagebreak   

```{r}
library(visdat)
library(car)
library(corrplot)
library(e1071)
library(tidyverse)

#Set default theme for ggplot
theme_set(theme_light()) 
```



# Executive Summary   

|       Capital Bike Share provides a network of multi-purpose bicycles to the denizens of the Washington D.C. Metropolitan region. We were approached by Capital Bike Share to delve into their hour-by-hour observations across the 2011 and 2012 time frame. Preliminary insights into the dataset provided to us revealed that demand usage for these bicycles can be affected by a variety of noteworthy, influential factors. These can be broken down into two major categories of influence: time and weather.   

|        We examined multi-variable trends and deliberated on patterns which revealed key insights. Based on our visualizations, we concluded rental demand as being dominated by registered riders using bicycles as a primary mode of transportation to and from work. Furthermore, our resulting predictive model confirmed our preliminary analysis, as it was ultimately determined that time-based variables were the most significant factors in accurately predicting bicycle rentals. 

|       

# Problem Statement and Approach

Our primary tasks in this analysis were twofold:   

- Identifying the most influential variables relative to their predictive power in determining the total hour-by-hour bicycle rentals and,  
- Fitting a multiple regression model predicting the demand for the total hour-by-hour bicycle rentals.

|       Beginning with an inspection of our dataset and subsequent exploratory data analysis, we aimed to systematically determine those variables which we believed have significant relation to the target variable. Our dataset consisted of 17,379 observations across 17 features. After a well-documented, granular analysis, including univariate and multivariate visualizations, we utilized our findings in a comprehensive modeling process. This process culminated in a multiple linear regression model that not only accurately predicted rental demand, but simultaneously provided an assumption-backed evaluation confirming our models' reliability.

|       Below we have provided the methodology of our approach, beginning with an inspection and analysis of our data, followed by our modeling selection strategy and diagnostic testing to ensure model generalizability. Finally, we conclude with our recommendations and takeaways.    


# Methodology

## Data Preprocessing     

|       We began our approach with an overview of the integrity of our dataset's structure. We discovered no missing values nor duplicated observations. Neither imputation nor duplicate observation removal was needed. We did, however, note variables which had data types that were non-representative of the underlying values

Variables `dteday`, `season`, `yr`, `mnth`, `hr`, `holiday`, `weekday`, `workingday`, and `weathersit` were all considered to have data types and values which were unclear and in need of re-evaluation. As such, we decided to apply new data types to these variables, effectively categorizing them and applying appropriate labels. The `dteday`, `yr`, `mnth`, `hr`, `weekday`, `weathersit`, and `cnt` were renamed for additional clarity.

```{r}
#Read In Dataset & Initial Look
CBS <- read.csv("Capital Bike Sharing data by hour.csv")
CBS %>% glimpse()
``` 

```{r}
#Rename Variables for descriptive clarity
CBS <- CBS %>% rename(date = dteday,
                      year = yr,
                      month = mnth,
                      hour = hr,
                      day = weekday,
                      weather = weathersit,
                      count_rentals = cnt)
CBS <- CBS %>% dplyr::select(-instant)
CBS %>% glimpse()
```

```{r}
#Visualize for Missing Data
fig.1 <- vis_miss(CBS, sort_miss = T) + 
  labs(y = "NA Values") +
  theme(axis.text.x.bottom = element_text(angle = 60, vjust = 0)) +
  coord_flip()
```

```{r}
#Distinct Row Check - Exclude ID from search
distinctCheck <- CBS[,2:ncol(CBS)]
duplicates <- CBS[which(duplicated(distinctCheck)), ]
if ((nrow(duplicates)) == 0) {
  print("No duplicates detected") }
```

```{r fig.height=4, fig.width=6}
fig.2 <- vis_dat(CBS) + theme(axis.title.y = element_blank(),
                     axis.text.x.bottom = element_blank()) +
  labs(title = "Data Types of Capital Bike Share Dataset") +
  scale_fill_brewer(palette = 2)
```


```{r}
#Reevaluate data types
CBS$date <- as.Date(CBS$date)
CBS$season <- factor(CBS$season, labels = c("Winter", "Spring", "Summer", "Fall"))
CBS$year <- factor(CBS$year, labels = c("2011", "2012"))
CBS$month <- factor(CBS$month, labels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                       "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))
CBS$holiday <- as.logical(CBS$holiday)
CBS$day <- factor(CBS$day, labels = c("Sunday", "Monday", "Tueday", "Wednesday", "Thursday", "Friday", "Saturday"))
CBS$workingday <- as.logical(CBS$workingday)
CBS$weather <- factor(CBS$weather, labels = c("Type 1", "Type 2", "Type 3", "Type 4"))

#Function - Redefine Int Hour Values to Factor Values
reHour <- function(hourInteger) {
  stringHour = ""
  if (hourInteger == 0) {
    return ("12:00AM")
  } else if (hourInteger > 0 & hourInteger < 12) {
    stringHour = paste(hourInteger, ":00AM", sep = "")
  } else if (hourInteger == 12) {
    return ("12:00PM")
  } else {
    hourInteger = hourInteger - 12
    stringHour = paste(hourInteger, ":00PM", sep = "")
  }
  return (stringHour)
}

#Retrieve New Values
newHours <- c()
for (i in 1:nrow(CBS)) {
  newHours <- c(newHours, reHour(CBS$hour[i]))
}
CBS$hour <- factor(newHours)
```


```{r fig.height=4, fig.width=6}
#Visualize final data types in dataset
vis_dat(CBS) + theme(axis.title.y = element_blank(),
                     axis.text.x.bottom = element_blank()) +
  labs(title = "Data Types of Capital Bike Share Dataset") +
  scale_fill_brewer(palette = 2)
```

```{r}
### Current Dimensionality of Capital Bike Share Dataset
print(paste("Number of observations/rows: ", dim(CBS)[1]))
print(paste("Number of columns/variables: ", dim(CBS)[2]))
```

## Exploratory Data Analysis    

|       To better understand our variables' distributions, values, and relationships with each other and our target variable, we conducted a thorough exploratory data analysis. Our primary focus is to examine each of our variables' summary statistics and distributions while investigating the underlying values for patterns, inconsistencies, and anomalies which may affect our analysis. 

### Univariate Analysis 

#### Target Variable   

```{r, warning = FALSE}
#Summary Statistics for Continuous Variables
numSummary = function(var, rowname) {
  tab = data.frame(Mean = mean(var) %>% round(2),
           Median = median(var),
           StdErr = sd(var) %>% round(2),
           Skew = skewness(var) %>% round(2),
           Q1 = quantile(var, .25),
           Q3 = quantile(var, .75),
           IQR = IQR(var),
           Min = min(var),
           Max = max(var))
  rownames(tab) = rowname
  return(tab)
}
```

##### `count_rentals`  

|       We are focused primarily on finding relationships significant to and in predicting the target variable, `count_rentals`(previously `cnt`). Each row/observation in our dataset is taken on an hourly basis. Our target is that corresponding hourly count of rentals that have occurred. Our histogram displaying the distribution overlayed with a density line shows us graphically that `count_rentals` is moderately right skewed. There is serious variability in the hour-by-hour rentals, with a minimum of a single rental to a maximum of nearly a thousand rentals per hour, but with 50% of our observations being less than **142** bicycle rentals per hour, marked by the red dashed line. 

```{r}
#Retrieve summary statistics for count_rentals
results = numSummary(CBS$count_rentals, "count_rentals")
print(results)

#count_rentals distribution
CBS %>% ggplot(aes(count_rentals)) +
  geom_histogram(aes(y = after_stat(density)),fill = "darkblue", color = "black", alpha = 0.6) +
  geom_density(adjust = 2) +
  labs(x = "Total Number of Rentals", y = "Density",
       title = "Distribution of `count_rentals`",
       subtitle = paste("Skewness Value: ", results$Skew)) +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")
  ) +
  geom_vline(aes(xintercept = results$Median), color = "red", linetype = "dashed")
```

#### Categorical Variables  

##### `date`

The `date` distribution revealed that not every unique value of date has an expected equivalent number of hourly interval measurements. When considering the **731** distinct date values present in our dataset, the majority of each contained twenty-four or twenty-three of the expected hourly observations. However, fourteen of the dates contained less and in some cases contained a mere single hourly observation, which in total equated to **233** hours worth of missing observations.  While the fact that these distinct dates have missing hour-to-hour observations, it does not immediately follow that these observations are necessarily outliers. However, it should be noted that Capital Bike Shares' hour-to-hour observational system has hourly gaps, which would otherwise provide useful descriptive and predictive analytics. 

```{r}
# Distinct Values per Variable
print(paste("Number of Distinct Dates: ", n_distinct(CBS$date)))

#Observe Low Frequency of some particular dates
low_count_obs <- CBS %>% group_by(date) %>%
  summarise(count_hour_obs = n()) %>%
  filter(count_hour_obs < 23)

print(low_count_obs)
print(paste(sum(low_count_obs$count_hour_obs), "hours worth of missing observations."))
```

```{r fig.height=4, fig.width=6}
#Display those date in distribution with fewer counts of hourly observations
CBS %>% group_by(date) %>% 
  mutate(dateCount = n()) %>% 
  filter(dateCount < 23) %>%
  ggplot(mapping = aes(x = fct_reorder((as.character(date)), desc(dateCount)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(date), desc(dateCount)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 1, size = 2) +
  scale_fill_viridis_d("Date", option = "magma") +
  guides(fill = "none") +
  labs(x = "Date",
       caption = "Dates on the right side of the chart, such as 2012-10-29 and 2011-01-27, correspond to very few observations in the entire data set.",
       y = "Number of Observations",
       title = "Dates with Missing Hourly Observations") +
  theme(axis.text.x = element_text(angle = 35),
        plot.caption = element_text(size = 6))
```
 
##### `season`   

The seasonal distribution regarding the number of observations between Fall, Winter, Spring, and Summer revealed only a slight imbalance. Fall contained the fewest of these with 4232 observations, Winter with 4242 observations, Spring with 4409 observations, and Summer contained the most with 4496 observations. As a whole, the  distribution was evenly distributed with each season containing approximately within +/- 1% of a quarter of the observations as would be expected. 

```{r}
print(paste("Percentages of seasons:"))
(prop.table(table(CBS$season)) * 100) %>% round(2)

CBS %>% 
  group_by(season)%>%
  mutate(countSeason = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((season), desc(countSeason)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(season), desc(countSeason)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 200, size = 2) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `season`",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
```

##### `year`    

The distribution of observations for the two recorded years, 2011 and 2012, were nearly exact at 49.74% and 50.26%, respectively.

```{r}
print(paste("Percentages of Observations for years:"))
(prop.table(table(CBS$year)) * 100) %>% round(2)

CBS %>% 
  ggplot(mapping = aes(x = year, fill = year)) +
  geom_bar(color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 450, size = 2) +
  scale_fill_viridis_d("Year", option = "magma") +
  labs(title = "Distribution of Observations by `year`",
       x = "",
       y = "") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  guides(fill = "none") +
  coord_flip()
```

##### `month`    

Across all monthly grouped observations, the distribution of the `month` variable was approximately uniform. However, the month of February appeared unique with it having the fewest number of observations of 1341. May had the highest count of recorded observations with 1488 recorded. All other months' counts of observations fell within these two counts of 1341 to 1488 (about 8% each of total observations). 

```{r fig.width=4.7}
print(paste("Percentages of months:"))
(prop.table(table(CBS$month)) * 100) %>% round(2)

CBS %>% 
  group_by(month) %>%
  mutate(countMonth = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((month), desc(countMonth)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(month), desc(countMonth)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 75, size = 2) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `month`",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
``` 


##### `hour`    

When observing a sorted hourly distribution from 12:00AM to 11:00PM, we found that the count of each distinct hour were nearly equal. However, a trend in the number of observations can be seen with a decrease beginning at approximately 1:00AM and continuing until 3:00AM when it then increases through 6:00AM. 

```{r fig.width=5}
#hour
print(paste("Percentages of Hourly Observations:"))
(prop.table(table(CBS$hour)) * 100) %>% round(2)

hours <- CBS$hour %>% unique() %>% as.vector()
hourDF <- CBS %>% 
  group_by(hour) %>% 
  transmute(hourCount = n()) %>% 
  mutate(check = ifelse(hourCount <= 725, TRUE, FALSE))

hourDF %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours))) +
  geom_bar(color = "black", aes(fill = check), alpha = .4) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 40, size = 2.5) +
  labs(title = "Distribution of `hour`",
       x = "",
       y = "") +
  scale_fill_viridis_d()+
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    plot.caption = element_text(size = 8),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(angle = 85, vjust = 0.8),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none")
```

##### `day`     

The distribution across the `day` variable were all nearly approximate for each of the seven days of the week.

```{r}
print(paste("Percentages of days:"))
(prop.table(table(CBS$day)) * 100) %>% round(2)

CBS %>% 
  group_by(day) %>%
  mutate(countDay = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((day), desc(countDay)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(day), desc(countDay)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 120, size = 2) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `day`",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
```

##### `weather`    

We noted an immediate yet unsurprising distribution with the `weather` variable. A vast majority of our observations were of Type 1, meaning that a majority of our observations were recorded in favorable weather conditions. The second largest proportion of observations were of Type 2 which included cloudy and misty weather. The third largest were of Type 3 which indicates rain/snow and thunderstorm conditions. The smallest proportion were those recorded for Type 4. These observations were of extreme weather conditions, including heavy rain/snow, severe thunderstorms/etc.

```{r}
#weather
print(paste("Percentages of weather Types:"))
(prop.table(table(CBS$weather)) * 100) %>% round(2)

CBS %>% 
  group_by(weather) %>%
  mutate(countWeather = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((weather), desc(countWeather)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(weather), desc(countWeather)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 500, size = 2.5) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `weather`",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    plot.subtitle = element_text(size = 8),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
```

#### Continuous Variables  


Note: Additional summary statistics for each of the continuous variables can be found in the Appendix in Figure 6. Each of our continuous variables were pre-normalized which in turn will help us determine their importance in predicting the target with respect to one another. Additionally, each continuous variables distribution can be found in Figure X.

##### `temp`     

The distribution of `temp` has been normalized to represent the actual temperature in Celsius. 

```{r}
#Retrieve summary statistics for temp
tempSummary = numSummary(CBS$temp, "temp")
numVars = bind_rows(results, tempSummary)
tempSummary

#temp Distribution
CBS %>% ggplot(aes(temp)) +
  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = "blue", color = "black", alpha = 0.6) +
  geom_density(adjust = 3) +
  labs(x = "Normalized Celcius",
       y = "Density", 
       title = "Distribution of `temp`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))
```


##### `atemp`   

The distribution of `atemp` has been normalized and represents the "real-feel" temperature in Celsius.  Being that `atemp` is undoubtedly related to influences from the season and weather, our intuition tells us that `atemp` will be an important factor in determining when people will be likely to rent Capital Bike Share bicycles.

```{r}
#Retrieve summary statistics for atemp
tempFCSummary = numSummary(CBS$atemp, "atemp")
numVars = bind_rows(numVars, tempFCSummary)
tempFCSummary

#atemp Distribution
CBS %>% ggplot(aes(atemp)) +
  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = "blue", color = "black", alpha = 0.6) +
  geom_density(adjust = 3) +
  labs(x = "Normalized - Real Feel Celcius",
       y = "Density", 
       title = "Distribution of `atemp`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))
``` 

##### `hum`    

Despite the distribution of `hum` representing humidity being normalized, we see that a majority of the observations contain some degree of humidity. Being that `hum` possibly includes rain, snow, or other precipitation, we might see that it negatively impacts the number of rentals. 

```{r}
#Retrieve summary statistics for humidity
humSummary = numSummary(CBS$hum, "hum")
numVars = bind_rows(numVars, humSummary)
humSummary

#Humidity Distribution
CBS %>% ggplot(aes(hum)) +
  geom_histogram(aes(y = after_stat(density)), fill = "blue", color = "black", alpha = 0.5) +
  geom_density(adjust = 3) +
  labs(x = "Normalized Humidity",
       y = "Density", 
       title = "Distribution of `hum`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))
```

##### `windspeed`      

We have a uniquely zero-inflated, skewed distribution with the `windspeed` variable. Quite a few of our observations are marked as having zero `windspeed`, with a notable gap between zero and the next marked observation. It may be the case that these readings are of truly zero `windspeed` or that the wind was so negligible within this gap of ranges that they were marked as zero. 

```{r}
#Retrieve summary statistics for windspeed
windSummary = numSummary(CBS$windspeed, "windspeed")
numVars = bind_rows(numVars, windSummary)
windSummary

#Windspeed Distribution
CBS %>% ggplot(aes(windspeed)) +
  geom_histogram(aes(y = after_stat(density)), fill = "blue", color = "black", alpha = 0.5) +
  geom_density(adjust = 3) +
  labs(x = "Normalized Windspeed",
       y = "Density", 
       title = "Distribution of `windspeed`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))
```

```{r}
#Summary Statistics of Continuous Variables 
library(DT)
datatable(numVars, class = 'cell-border stripe')
```

#### Boolean(True/False) Variables 

##### `holiday`   

Inspection of our `holiday` variable revealed dates which should have been marked as holidays and others which should not have been. We re-labeled these observations based on official [federally recognized holidays](https://www.commerce.gov/hr/employees/leave/holidays).    

In comparison to our dataset: **2011-01-01, 2011-12-25, 2012-01-01, and 2012-11-11** were incorrectly mislabeled as *not* being holidays.  **2011-12-26, 2012-01-02, and 2012-11-12** were mislabeled as **being** holidays. The aforementioned dates were corrected as to maintain consistency with federally recognized holidays. **2011-04-15 and 2012-04-16**, while not being federally recognized holidays, are dates of observance for Emancipation Day in the Washington D.C. Area. Due to the similar holiday-like observance of Emancipation Day in our area of interest, these two dates will be labeled as holidays.   

The final distribution remains highly disproportionate yet expected, with only 525 of 17379 observations being labeled as holidays.


```{r} 
holidayDates <- CBS[CBS$holiday == TRUE,]
holidayDates$date %>% unique()
```

```{r} 
#2011 Holiday Dates
USFed2011 <- c("2011-01-01", "2011-01-17", "2011-02-21","2011-04-15", "2011-05-30", "2011-07-04", "2011-09-05", "2011-10-10", "2011-11-11", "2011-11-24", "2011-12-25")
incHolidayDates2011 <- CBS[CBS$date %in% USFed2011 & CBS$holiday == FALSE,]$date %>% unique()

#2012 Holiday Dates
USFed2012 <- c("2012-01-01", "2012-01-16", "2012-02-20", "2012-04-16", "2012-05-28", "2012-07-04", "2012-09-03", "2012-10-08", "2012-11-11", "2012-11-22", "2012-12-25")
incHolidayDates2012 <- CBS[CBS$date %in% USFed2012 & CBS$holiday == FALSE,]$date %>% unique()

#Merge incorrect dates
incHolidays <- c(incHolidayDates2011, incHolidayDates2012)
print("Should be labeled as holidays: ")
print(incHolidays) 

#Combined Holiday List 
combinedUSFedHolidays <- c(USFed2011, USFed2012)
print("Combined List of Federally Recognized U.S. Holidays for 2011/2012: ")
print(combinedUSFedHolidays)

#Correct Holiday Labels
CBS<- CBS %>% mutate(holiday = ifelse(date %in% c(combinedUSFedHolidays), TRUE, FALSE))
```

```{r fig.width=5}
print(paste("Percentages of holiday Observations:"))
(prop.table(table(CBS$holiday)) * 100) %>% round(2)

#Holiday Distribution
CBS %>% 
  ggplot(mapping = aes(x = holiday, fill = holiday)) +
  geom_bar(color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 1000, size = 3) +
  scale_fill_viridis_d("Year", option = "magma") +
  labs(title = "Distribution of `holiday`",
       x = "",
       y = "") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  guides(fill = "none") +
  coord_flip()
```

##### `workingday`   

The distribution of `workingday` reveals that we have many more observations consisting of days in which people worked than not. This is consistent when considering that no Saturday, Sunday, weekday/end holidays can be working days.  

```{r}
print(paste("Percentages of workingday Observations:"))
(prop.table(table(CBS$workingday)) * 100) %>% round(2)

#Holiday Distribution
CBS %>% 
  ggplot(mapping = aes(x = workingday, fill = workingday)) +
  geom_bar(color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 1000, size = 3) +
  scale_fill_viridis_d("Year", option = "magma") +
  labs(title = "Distribution of `workingday`",
       x = "",
       y = "") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  guides(fill = "none") +
  coord_flip()
```

## Correlation   

We decided to investigate the extent to which our variables are linearly related to one another with a visualized correlation matrix~[Figure XX]~. Here we are concerned primarily with those relationships related to that of our target, `count_rentals` and continuous predictor variables. Here we list the most noteworthy relationships:  


- Being that `temp` and `atemp` both aim to measure normalized Celsius, it is unsurprising that these variables exhibit a near exact linear relationship with one another. We additionally see they both are moderately, positively related to our target variable to the same degree. Naively including both `temp` and `atemp` in our modeling process would result in multicollinearity, in which we have variables competing to explain our target variable with redundant information. This can cause our estimates our model coefficients to become unstable. To maintain a robust regression model, we will choose only one of these variables to predict the number of rentals.  

- Our target is to some degree negatively correlated with `hum`, or the humidity. Being that humidity includes potential precipitation such as rain, mist, snow, and others, it is unsurprising we might find that `count_rentals` decreases as `hum` increases.   

Overall, we see that each of our continuous predictor variables exhibits straight-line relationship with our target variable. This can be seen by examining the bottom row of Figure XX as indicated by scatter-plots and the red trend line. 

```{r  fig.height=6, fig.width=6}
#Show relationships between predictors and dvs
pred_CBS <- CBS %>% dplyr::select(-c(registered, casual))
cor_matrix <- cor(pred_CBS[sapply(pred_CBS, is.numeric)])

# Visualize the correlation matrix using the corrplot package
corrplot(cor_matrix, method = "circle",
         type = "upper", order = "AOE", 
         tl.col = "black", tl.cex = 1,
         addCoef.col = "darkred",
         number.cex = 1, tl.srt = 50)
```


```{r  fig.height=8, fig.width=8}
library(PerformanceAnalytics)
corr_df <- pred_CBS %>% select_if(is.numeric)
chart.Correlation(corr_df)
```

## Notable Multivariate Analysis  

### Temperature and Target Relationship

We examined the multi-variable relationship between our temperature related variables, `temp`,`atemp`, and `count_rentals`. Based on our graph, we found that unsurprisingly, the highest numbers of rentals occurred in normalized Celsius values we believe to be indicative of favorable conditions.     

This plot also revealed twenty-four observations having the exact same `atemp` value of **0.2424**, but `temp` values which were relatively high and variable in relation, ranging from **0.62 to 0.86**. Delving into these observations, we found that all were recorded sequentially, on the exact same day(*2012-08-17*) and under the same weather conditions(*Type 1*). The changing levels of humidity and wind speed lead us to further believe that the real feel temperature should have likewise varied when compared to other observations of similar values in the dataset.   

```{r fig.height=4.5, fig.width=5}
CBS %>% ggplot(aes(atemp, temp)) +
  geom_raster(aes(fill = count_rentals)) +
  labs(x = "Real Feel - Celcius",
       y = "Actual Temp - Celcius",
       title = "Rentals Relationship to Temperature Real & Feel",
       subtitle = "Highest Occurrance During Favorable Degrees of Celcius") +
  scale_fill_viridis_c(option = "magma") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  geom_rect(aes(xmin = .21, ymin = .55, xmax = .27, ymax = .94), color = "red", fill = NA) +
  geom_text(x = .20, y = .5, label = "Outliers", color = "red")
```

When considering the evidence of these twenty-four `temp` and `atemp` multivariate as being observations, we will opt to remove these observations. This removal helps to ensure data integrity as we move towards the modeling process. 

```{r fig.height=4.5, fig.width=5}
#Examine multi-variate outliers to find potential reason of high `temp` values
mvOutliers <- CBS[CBS$atemp < .3 & CBS$temp > .5,]
mvOutliers

#Remove Outlier Observations
subset <- (!(rownames(CBS) %in% rownames(mvOutliers)))
CBS <- CBS[subset, ]

rownames(CBS) <- NULL
```


### Hourly Rental Trends 

Due to the widespread availability of bicycles as a mode of transportation at all hours of the day, our intuition led us to investigate possible patterns of usage based solely on time. What we uncovered, showed that during working days, there is strong indication that bicycles are being utilized as a primary mode of transportation to and from work. Most notably, it is Capital Bike Shares `registered` rental users that are driving this trend. We see that between all rentals and only registered, the trend is nearly identical~[Figure XX, Figure XX]~. Neither `casual` rentals nor rentals occurring on Saturday/Sunday rentals exhibit this trend of bicycle usage, adding further evidence to our speculation~[Figure XX, Figure XX]~. 

It may be the case that Capital Bike Share bicycles are used as a primary mode of transportation for the majority of `registered` riders during working hours. Before 9:00AM, we note an upward trend of rentals followed by a stark decrease thereafter. We interpret this as individuals arriving at work followed by a sharp increase of usage around 4:00PM/5:00PM when people typically leave work and commute home. 

```{r fig.width=8, fig.height = 10} 
#Examining the relationship between day of the week and hourly trends
hours <- CBS$hour %>% unique() %>% as.vector()

#Usage looks identical for casual & registered --> therefore only showing count_rentals
CBS[(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), count_rentals, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Total Rentals",
       title = "Weekend Hourly Trend of Rentals",
       subtitle = "Smooth Curve of Rental with Peak Usage Around Midday")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 25,
                                   size = 9),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)

#Weekday Trend Over Hours - Casual
CBS[!(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), casual, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Total Rentals",
       title = "Weekday Hourly Trend of Casual Rentals")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 25,
                                   size = 9),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)
```

```{r fig.width=8, fig.height = 10} 
#Weekday Trend Over Hours - Registered
CBS[!(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), registered, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Total Rentals",
       title = "Weekday Hourly Trend of Registered Rentals",
       subtitle = "Peak Usage Before & After Work 9-5 Schedule")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 25,
                                   size = 9),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)

#Weekday Trend Over Hours - Total
CBS[!(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), count_rentals, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Total Rentals",
       title = "Weekday Hourly Trend of Total Rentals",
       subtitle = "Peak Usage Before & After Work 9-5 Schedule")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 25,
                                   size = 9),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)
```

## Feature Reduction   

`date` - Our team reasoned that the creation of **731** distinct variables representing `date` could have severe impacts, mainly high VIF and overfitting, on model performance when regressing our target variable. No unique information is provided by `date`. All other information is already found in our `year` and `month`, and `day` variable.  

To avoid these drawbacks, we have decided to disregard the `date` variable from the model building process.   

`temp` - As noted before, multicollinearity would be present in our model if we including variables `temp` and `atemp`. Therefore, we have decided to disregard `temp` from the modeling process, as we believe that `atemp` will more accurately prove useful in accurately predicting the target variable.  

## Feature Engineering

Dummy Variable Creation - Categorical variables, such as `season`, `year`, `month`, `day`, `hour`, and `weather`, are better suited for dummy variable creation for use in our multiple linear regression model. This encoding ensures these factor/categorical variables are appropriately interpreted by our model. For each of these variables, one of the categories will not appear in the model summary output, due to it being considered the baseline. 

```{r}
#Create Dummy Coding here for these variables 
library(fastDummies)

#Create Dummy Variables
CBS <- dummy_cols(CBS, select_columns = 
                    c("season", "year", "month", "day", "hour", "weather"),
                  remove_first_dummy = T) %>%
  dplyr::select(-c(season, year, month, day, hour, weather))

#Coerce Logical Types to Numeric
CBS <- CBS %>% mutate_if(is.logical, as.numeric)
CBS <- CBS %>% dplyr::select(-c("casual", "registered"))
CBS <- CBS %>% dplyr::select(-c(date, temp))
```

# Model Building  

## Data Partition    

With our goals centered on finding the most significant predictors with respect to determining the total number of hour-by-hour rentals, we decided to partition our data 80% towards training the model and 20% for testing model performance. 

```{r}
library(leaps) #Best Subset Selection
library(caret) #data split
#For Reproducibility
set.seed(123)
#80/20 Test Split 
index <- createDataPartition(CBS$count_rentals, p = .8, list = FALSE)
train <- CBS[index, ]
test <- CBS[-index, ]
rownames(train) <- 1:nrow(train)
rownames(test) <- 1:nrow(test)
```

## Initial Full Model(s)   

```{r, fig.width=6, fig.height=6}
full_model <- lm(count_rentals ~ ., data = train)
summary(full_model)
```

### Model Assumptions  

```{r, fig.width=6, fig.height=6}
par(mfrow = c(2,2)) 
plot(full_model)
```

#### Normality

```{r}
# Kolmogorov-Smirnov Test - Normality Assumption 
ks <- ks.test(full_model$residuals, 'pnorm')
if (ks$p.value <= 0.05) {
  print("H0 rejected: the residuals are NOT distributed normally")
} else {
  print("H0 failed to reject: the residuals ARE distributed normally")
}
ggplot(mapping = aes(x= full_model$residuals)) + 
  geom_histogram(fill = "white", color = "black") +
  labs(x = "Residuals", y = "Frequency",
       title = "Histogram of Residuals")
```

#### Homoscedasticity 

```{r fig.height=6, fig.width=6}
#Breush-Pagan Test - Homoscedasticity Assumption
library(lmtest)
bp <- bptest(full_model)
  if (bp$p.value <= 0.05) {
    print("H0 rejected: Error variance spreads INCONSTANTLY/generating patterns (Heteroscedasticity)")
  } else {
    print("H0 failed to reject: Error variance spreads CONSTANTLY (Homoscedasticity)")
  }
plot(full_model, which = 1)
```

#### Multicollinearity  

```{r}
library(car)
df_vif = vif(full_model) %>% data.frame()
colnames(df_vif) = "VIF"
df_vif %>% arrange(desc(VIF))
```

### Model Improvement

#### Model Outliers/Influential Observations   

Each of our three `weather_type_4` observations had significantly large leverage on our model fitting. Because of this we decided it would be best to remove these. Because of this, the `weather_type_4` variable consequently was removed from the training data set. 

```{r fig.height=12, fig.width=12}
#Rownames were reset during train/test split
out_lev <- c(450, 7302, 7093)
train <- train[-c(out_lev), ]
train <- train %>% dplyr::select(-`weather_Type 4`)
```

#### Target Variable Transformation 

[REWRITE]

during this analysis is to fit multiple linear regression models that carry certain assumptions which must be satisfied. These mainly include our response variable being linearly related to the predictors(linearity) and equality of variances in the residuals(homoscedasticity). With their current skewed distributions, these two assumptions would undoubtedly be violated post-model building.

In order to satisfy the assumptions of linear regression, a transformation on each of the target variables becomes necessary. We considered multiple transformations to approach normality, including, the Box-cox, logarithmic, square-root, cube-root, and fourth-root transformations. For each of our target variables, the transformation which approximated normality (closest skewness value to 0) the closest was the **cube-root** transformation. Neither the logarithmic and Box-cox transformations could not be applied to `casual` and `registered` due to the presence of zero(0) values which are incompatible with the transformations. 

```{r}
library(MASS)
#Box-cox Transformation on count_rentals
b <- boxcox(lm(CBS$count_rentals ~ 1), plotit = F, lambda = seq(-3,3, .05))
lambda <- b$x[which.max(b$y)]
box_total <- (CBS$count_rentals ^ lambda - 1) / lambda
#Log Transformation
log_total <- log(CBS$count_rentals)
#Sqrt Transformation
sqrt_total <- sqrt(CBS$count_rentals)
#Cube Root Transformation
cube_root_total <- (CBS$count_rentals)^(1/3)
#Fourth Root Transformation
fourth_root_total <- (CBS$count_rentals)^(1/4)

ggTrans <- function(var, myTitle) {
  #Cube Root Transformation - count_rentals
  skewVal <- skewness(var) %>% round(3)
  CBS %>% ggplot(aes(var)) +
    geom_histogram(aes(y = after_stat(density)),fill = "darkblue", color = "black", alpha = 0.6) +
    geom_density(adjust = 2) +
    labs(x = "Cube Root Total", y = "Density",
         title = myTitle,
         subtitle = paste("Skewness Value: ", skewVal)) +
    theme(
      plot.title = element_text(hjust = 0, face = "bold", size = 12),
      axis.title.x = element_text(size = 8, color = "black"),
      axis.title.y = element_text(size = 8, color = "black"),
      axis.text.x = element_text(size = 8, color = "black"),
      axis.text.y = element_text(size = 8, color = "black"))
}
```

```{r}
#Inspect Distributions
ggTrans(cube_root_total, "Cube Root Transformation")
ggTrans(box_total, "Box-Cox Transformation")
ggTrans(fourth_root_total, "Fourth Root Transformation")
ggTrans(sqrt_total, "Square Root Transformation")
ggTrans(log_total, "Log Transformation")
```

```{r}
print("---Best Transformation Approaches---")
print(paste("cube_root_total skewness value: ", skewness(cube_root_total)))
```

```{r}
#Add cube-root transformations to the dataset
train$cube_root_total <- (train$count_rentals)^(1/3)
test$cube_root_total <- (test$count_rentals)^(1/3)
```

#### Multicollinearity (VIF) 

```{r, fig.width=7, fig.height=7}
#Could remove workingday & season_Summer from train df
train <- train %>% dplyr::select(-c( workingday, season_Summer))
```

```{r, fig.width=7, fig.height=7}
# Model - Assumptions Addressed
full_model_improved <- lm(cube_root_total ~ . - count_rentals,
                          data = train)
par(mfrow = c(2,2))
plot(full_model_improved)
summary(full_model_improved)
df_vif = vif(full_model_improved) %>% data.frame()
colnames(df_vif) = "VIF"
df_vif %>% arrange(desc(VIF))
```

## Stepwise Selection (Reduced Model)   

### Model Assumptions  

### Test Set Evaluation  

```{r fig.height=5, fig.width=8.5}
library(leaps)

OLS.regback <- leaps::regsubsets(cube_root_total ~ . - count_rentals, train, method = "seqrep", nvmax = 80)

OLSregbacksum <- summary(OLS.regback)

adjr2mat <- data.frame(t(OLSregbacksum$adjr2))
bicmat <- data.frame(t(OLSregbacksum$bic))

combined_df <- data.frame(
  Adj_R2 = unlist(adjr2mat),
  BIC = unlist(bicmat)
)

combined_df[35:45,]

par(mfrow = c(1,2))
plot(OLSregbacksum$adjr2, xlab = "Number of Variables",
    ylab = "Adjusted RSq", type = "l", lwd = 1.5, main = "Number of Variables Vs. Adjusted R2",
    cex.main = 1.15, cex.lab = 1, cex.axis = 1.05, font.axis = 2,
    font.lab = 2, panel.first = grid(nx = NULL, ny = NULL, col = "gray",
        lty = 2))
points(36, OLSregbacksum$adjr2[36], col = "darkred", cex = 2, pch = 20)

plot(OLSregbacksum$bic, xlab = "Number of Variables", ylab = "BIC",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. BIC",
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
points(36, OLSregbacksum$bic[36], col = "darkred", cex = 2, pch = 20)
```

```{r}
# Find the model with the lowest BIC value
best_model_index <- which.min(OLSregbacksum$bic)

# Get the best model
best_model <- coef(OLS.regback, id = best_model_index)

# Extract the coefficients from the best model and remove the intercept term from the coefficients
best_coefficients <- round(unname(best_model)[-1], 4)

# Get the variable names from your redDF dataframe
variable_names <- names(best_model)[-1]

# Extract the intercept coefficient separately
intercept_coefficient <- round(unname(best_model)[1], 4)

# Define the named_coefficients variable
named_coefficients <- setNames(best_coefficients, variable_names)
length(variable_names)
# Create the LaTeX equation
equation <- paste("y =", intercept_coefficient, "+", paste(named_coefficients, variable_names, sep = " * ", collapse = " + "), "")
```


```{r}
#---Prepare Final Dataset---  
#Extract the best stepwise variables from train
final_mod_vars <- named_coefficients %>% names()
vars <- train %>% names()

#Fix "`" and then return list of appropriate vars
variables_for_selection = c()
for (i in 1:length(final_mod_vars)) {
  var_to_list = str_replace_all(final_mod_vars[i], "`", "" )
  variables_for_selection = c(variables_for_selection, var_to_list)
}

#Prepare Final Training Dataset Based on Selected Variables
train_final <- train %>% select_if(vars %in% variables_for_selection)
train_final <- train_final %>% bind_cols(train$cube_root_total)
colname = train_final %>% names()
train_final <- train_final %>% rename(cube_root_total = colname[length(colname)])
```



```{r}
#Train Final Model - Success (Same coef as original)
stepwise_model <- lm(cube_root_total ~ .  ,data = train_final)

#Make Predictions
pred_values_total_rentals <- predict(stepwise_model, newdata = test) %>% data.frame()
pred_values_total_rentals$fit = (pred_values_total_rentals$.)^3
pred_values_total_rentals$obs = (test$cube_root_total)^3

#Return Metrics
metrics <- function(y_pred, y_true){
 rmse <- RMSE(y_pred, y_true)
 mse <- mean((y_pred - y_true)^2)
 mae <- MAE(y_pred, y_true)
 corPredAct <- cor(y_pred, y_true)
 df = data.frame(Root_Mean_Squared_Error = rmse,
                 Mean_Squared_Error = mse,
                 Mean_Absolute_Error = mae,
                 Corr_Between_Pred_Obs = corPredAct,
                 R_squared_Pred_Obs = corPredAct*corPredAct)
 df <- df %>% t() %>% data.frame()
 df <- df %>% rename(Test_Error_Metrics = ".")
 return (df)
}
model_metrics_df = metrics(y_pred = pred_values_total_rentals$fit,
                           y_true = pred_values_total_rentals$obs)
model_metrics_df
```



```{r fig.height=5, fig.width=5}
#Total Rentals Predicts vs. Observed Values
ggplot(pred_values_total_rentals, aes(x=fit, y=obs)) +
  geom_point(aes(color = abs(obs - fit)), size = 3, alpha = 0.6) + 
  geom_abline(intercept=0, slope=1, linetype="dashed", color="red") +
  scale_color_viridis_c(option = "B", direction = -1) + 
  labs(
    title = "Actual vs Predicted Number of Total Rentals",
    y = "Actual Number of Rentals",
    x = "Predicted Number of Rentals",
    color = "Residual Magnitude") +
  scale_x_continuous(breaks = seq(0, 1000, 250), labels = seq(0,1000,250)) +
   scale_y_continuous(breaks = seq(0, 1000, 250), labels = seq(0,1000,250))
```

```{r}
#Final Model Diagnostics
plot(stepwise_model)
summary(stepwise_model)
```

```{r}
#Test Error Metrics
model_metrics_df
```

```{r fig.height=12, fig.width=9}
library(coefplot)
coefplot::coefplot(stepwise_model)
```

# Conclusions and Recommendations 

Capital Bike Share Needs to Capitalize on the trend of registered riders using it as a primary mode of transportation and set of locations at major areas of work and near residential areas where people can get easy access to these bikes. This will increase the number of registered riders. 

\pagebreak

# Appendix 

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Visualize for Missing Data
vis_miss(CBS, sort_miss = T) + 
  labs(y = "NA Values") +
  theme(axis.text.x.bottom = element_text(angle = 60, vjust = 0)) +
  coord_flip()
```

```{r echo=FALSE, fig.align='center', fig.cap="No missing values in dataset", fig.show='hold', fig.width=7, include=TRUE}
fig.1
```

```{r include = TRUE, echo = TRUE, eval = FALSE}
#Visualize for Missing Data
vis_dat(CBS) + theme(axis.title.y = element_blank(),
                     axis.text.x.bottom = element_blank()) +
  labs(title = "Data Types of Capital Bike Share Dataset") +
  scale_fill_brewer(palette = 2)
```

```{r echo=FALSE, fig.align='center', fig.cap="Pre-Assigned Variable Data Types", fig.show='hold', fig.width=7, include=TRUE}
fig.2
```






