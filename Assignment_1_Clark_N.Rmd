---
title: "Capital Bike Share Predictive Model Report"
author: "Prepared By: Clark P. Necciai Jr."
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    number_sections: yes
    toc_depth: '3'
fontsize: 12pt
geometry: margin=1in
header-includes:
- \usepackage{setspace}
- \singlespacing
linkcolor: blue
---

```{r setup, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
#Default Setup
knitr::opts_chunk$set(include = FALSE, echo = FALSE, tidy=TRUE, cache=FALSE, message=FALSE, warning=FALSE,tidy.opts=list(width.cutoff=55, keep.blank.line=FALSE),fig.align='center', fig.width=3.50, fig.height=2.75, dpi = 300)
```

\pagebreak

```{r}
library(visdat)
library(car)
library(corrplot)
library(e1071)
library(tidyverse)
library(ggpubr)
library(DT)
#Set default theme for ggplot
theme_set(theme_minimal()) 
```

# Executive Summary

|       Capital Bike Share provides a network of multi-purpose bicycles to the denizens of the Washington D.C. Metropolitan region. We were approached by Capital Bike Share to delve into their hour-by-hour observations across 2011 and 2012. Preliminary insights into the dataset revealed that demand usage for these bicycles can be affected by a variety of influential factors. We determined two major categories of influence: time and weather.

|        We examined multi-variable trends and deliberated on insightful patterns. Based on our findings, we concluded rental demand as being mainly determined by registered riders using bicycles as a primary mode of transportation to and from work. Furthermore, our resulting predictive model confirmed our preliminary analysis, as it was ultimately determined that time-based variables were the most significant factors in accurately predicting bicycle rentals.

# Problem Statement and Approach

Our primary tasks in this analysis were twofold:

-   Identifying the most influential variables relative to their predictive power in determining the total hour-by-hour bicycle rentals
-   Fitting a multiple regression model predicting total hour-by-hour bicycle rental count

|       Beginning with an exploratory data analysis, we aimed to determine variables which we believed have significant relation to the target variable. After a well-documented, granular analysis of 17 features over 17,379 observations, we utilized our findings for comprehensive modeling. This process culminated in a multiple linear regression model that not only accurately predicted rental demand, but simultaneously provided an assumption-backed evaluation confirming our models' reliability and generalizability. We then end with our conclusions and recommendations.

# Methodology

## Data Preprocessing

|       We began our approach with an overview of the integrity of our dataset's structure. We discovered no missing values nor duplicated observations. We did, however, note variables which had data types that were non-representative of their underlying values.

|       Variables `dteday`, `season`, `yr`, `mnth`, `hr`, `holiday`, `weekday`, `workingday`, and `weathersit` were all considered to have data types and values which were non-representative. As such, we applied more representative data types, effectively categorizing them and applying appropriate labels. The `dteday`, `yr`, `mnth`, `hr`, `weekday`, `weathersit`, and `cnt` were renamed for additional clarity.

```{r}
#Read In Dataset & Initial Look
CBS <- read.csv("Capital Bike Sharing data by hour.csv")
#CBS %>% glimpse()
```

```{r}
#Rename Variables for descriptive clarity
CBS <- CBS %>% rename(date = dteday,
                      year = yr,
                      month = mnth,
                      hour = hr,
                      day = weekday,
                      weather = weathersit,
                      count_rentals = cnt)
CBS <- CBS %>% dplyr::select(-instant)
#CBS %>% glimpse()
```

```{r}
#Visualize for Missing Data
fig.1 <- vis_miss(CBS, sort_miss = T) + 
  labs(y = "NA Values") +
  theme(axis.text.x.bottom = element_text(angle = 60, vjust = 0)) +
  coord_flip()
```

```{r}
#Distinct Row Check - Exclude ID from search
distinctCheck <- CBS[,2:ncol(CBS)]
duplicates <- CBS[which(duplicated(distinctCheck)), ]
if ((nrow(duplicates)) == 0) {
  print("No duplicates detected") }
```

```{r}
fig.2 <- vis_dat(CBS) + theme(axis.title.y = element_blank(),
                     axis.text.x.bottom = element_blank()) +
  scale_fill_brewer(palette = 2)
```

```{r}
#Reevaluate data types
CBS$date <- as.Date(CBS$date)
CBS$season <- factor(CBS$season, labels = c("Winter", "Spring", "Summer", "Fall"))
CBS$year <- factor(CBS$year, labels = c("2011", "2012"))
CBS$month <- factor(CBS$month, labels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun",
                                       "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))
CBS$holiday <- as.logical(CBS$holiday)
CBS$day <- factor(CBS$day, labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
CBS$workingday <- as.logical(CBS$workingday)
CBS$weather <- factor(CBS$weather, labels = c("Type 1", "Type 2", "Type 3", "Type 4"))

#Function - Redefine Int Hour Values to Factor Values
reHour <- function(hourInteger) {
  stringHour = ""
  if (hourInteger == 0) {
    return ("12:00AM")
  } else if (hourInteger > 0 & hourInteger < 12) {
    stringHour = paste(hourInteger, ":00AM", sep = "")
  } else if (hourInteger == 12) {
    return ("12:00PM")
  } else {
    hourInteger = hourInteger - 12
    stringHour = paste(hourInteger, ":00PM", sep = "")
  }
  return (stringHour)
}

#Retrieve New Values
newHours <- c()
for (i in 1:nrow(CBS)) {
  newHours <- c(newHours, reHour(CBS$hour[i]))
}
CBS$hour <- factor(newHours)
```

```{r fig.height=4, fig.width=6}
#Visualize final data types in dataset
datatype_plot <- vis_dat(CBS) + theme(axis.title.y = element_blank(),
                     axis.text.x.bottom = element_blank()) +
  scale_fill_brewer(palette = 2)
```

```{r}
### Current Dimensionality of Capital Bike Share Dataset
print(paste("Number of observations/rows: ", dim(CBS)[1]))
print(paste("Number of columns/variables: ", dim(CBS)[2]))
```

## Exploratory Data Analysis (EDA)

|       To better understand our variables, we examined each of our variables' summary statistics and distributions while investigating for patterns, inconsistencies, and anomalies which may affect our analysis.

### Target Variable

```{r, warning = FALSE}
#Summary Statistics for Continuous Variables
numSummary = function(var, rowname) {
  tab = data.frame(Mean = mean(var) %>% round(2),
           Median = median(var),
           StdErr = sd(var) %>% round(2),
           Skew = skewness(var) %>% round(2),
           Q1 = quantile(var, .25),
           Q3 = quantile(var, .75),
           IQR = IQR(var),
           Min = min(var),
           Max = max(var))
  rownames(tab) = rowname
  return(tab)
}
```

#### `count_rentals`

The variable we aim predict is `count_rentals`(`cnt`). Each observation is taken on an hourly basis. Our target variable's distribution shows us that `count_rentals` is highly right skewed. There is serious variability in the hour-by-hour rentals, with a minimum of a single rental to a maximum of nearly a thousand rentals per hour, but with 50% of our observations being less than the median of **142** bicycle rentals per hour, marked by the red dashed line.

```{r}
#Retrieve summary statistics for count_rentals
results = numSummary(CBS$count_rentals, "count_rentals")
#print(results)

#count_rentals distribution
target_plot <- CBS %>% ggplot(aes(count_rentals)) +
  geom_histogram(aes(y = after_stat(density)),fill = "darkblue", color = "black", alpha = 0.6) +
  geom_density(adjust = 2) +
  labs(x = "Total Number of Rentals", y = "Density",
       title = "Distribution of `count_rentals`",
       subtitle = paste("Skewness Value: ", results$Skew),
       caption = "Median Marked By Dashed Red Line") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")
  ) +
  geom_vline(aes(xintercept = results$Median), color = "red", linetype = "dashed")
```



```{r}
# Distinct Values per Variable
#print(paste("Number of Distinct Dates:", n_distinct(CBS$date)))


#Observe Low Frequency of some particular dates
low_count_obs <- CBS %>% group_by(date) %>%
  summarise(count_hour_obs = n()) %>%
  filter(count_hour_obs < 23)

#Expected - Actual
lost_hours = (14*24) - sum(low_count_obs$count_hour_obs)
#print(paste("Hours Lost from Low Count Dates:", lost_hours))
#print(low_count_obs)
```

```{r fig.height=4, fig.width=7}
#Display those date in distribution with fewer counts of hourly observations
date_plot <- CBS %>% group_by(date) %>% 
  mutate(dateCount = n()) %>% 
  filter(dateCount < 23) %>%
  ggplot(mapping = aes(x = fct_reorder((as.character(date)), desc(dateCount)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(date), desc(dateCount)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 2, size = 4.4) +
  scale_fill_viridis_d("Date", option = "magma") +
  guides(fill = "none") +
  labs(x = "Date",
       y = "Number of Observations",
       title = "Dates with Missing Hourly Observations") +
  theme(axis.text.x = element_text(angle = 90),
        plot.caption = element_text(size = 8))
```

```{r fig.width=4.2}
season_plot <- CBS %>% 
  group_by(season)%>%
  mutate(countSeason = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((season), desc(countSeason)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(season), desc(countSeason)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 220, size = 3) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `season`",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
```

```{r fig.width=4}
year_plot <- CBS %>% 
  ggplot(mapping = aes(x = year, fill = year)) +
  geom_bar(color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 500, size = 3) +
  scale_fill_viridis_d("Year", option = "magma") +
  labs(title = "Distribution of `year`",
       x = "",
       y = "") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  guides(fill = "none") +
  coord_flip()
```

```{r fig.width=4.7}
month_plot <- CBS %>% 
  group_by(month) %>%
  mutate(countMonth = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((month), desc(countMonth)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(month), desc(countMonth)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 75, size = 3) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `month`",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
```

```{r fig.width=8}
hours <- CBS$hour %>% unique() %>% as.vector()
hourDF <- CBS %>% 
  group_by(hour) %>% 
  transmute(hourCount = n()) %>% 
  mutate(check = ifelse(hourCount <= 725, TRUE, FALSE))

hour_plot <- hourDF %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours))) +
  geom_bar(color = "black", aes(fill = check), alpha = .4) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 40, size = 3) +
  labs(title = "Distribution of `hour`",
       subtitle = "Slight Drop Trend in Count of Observations During Morning Hours",
       x = "",
       y = "") +
  scale_fill_viridis_d()+
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    plot.caption = element_text(size = 8),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(angle = 85, vjust = 0.6),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none")
```

```{r fig.width=4.5}
day_plot <- CBS %>% 
  group_by(day) %>%
  mutate(countDay = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((day), desc(countDay)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(day), desc(countDay)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 140, size = 3) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `day`",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
```

```{r fig.width=5}
#weather
weather_plot <- CBS %>% 
  group_by(weather) %>%
  mutate(countWeather = n()) %>%
  ggplot(mapping = aes(x = fct_reorder((weather), desc(countWeather)))) +
  geom_bar(color = "black", mapping = aes(fill = fct_reorder(as.factor(weather), desc(countWeather)))) +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 550, size = 3) +
  scale_fill_viridis_d("Month", option = "magma") +
  labs(title = "Distribution of `weather`",
       subtitle = "Few Instances of `Type 4` Weather/Inclement Weather Conditions",
       x = "",
       y = "Observations") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    plot.subtitle = element_text(size = 8),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"),
    ) +
  guides(fill = "none") +
  coord_flip()
```

### Predictor Variables

|       Each categorical, continuous, and Boolean features' summary statistics and distributions were thoroughly investigated. Categorical features we considered are as follows:  `season`, `year`, `month`, `day`, `hour`, `date`, and `weather`. Continuous features are `temp`, `atemp`, `hum`, and `windspeed`. Boolean features were `holiday` and `workingday`.

The distributions and corresponding summary statistics for the categorical, continuous, and Boolean variables can be observed in Figure X, Y, and Z respectively.

#### Noteworthy Observations

##### `date`

The `date` distribution revealed that not every unique value of date has an expected equivalent number of hourly interval measurements. The majority of each of the **731** distinct date values contained twenty-four or twenty-three of the expected hourly observations. However, fourteen of the dates contained fewer, which in turn equated to **103** hours worth of missing possible observations. Capital Bike Shares' hour-to-hour observational system has hourly gaps, which would otherwise provide useful statistics. 

```{r}
#Retrieve summary statistics for temp
tempSummary = numSummary(CBS$temp, "temp")
numVars = bind_rows(results, tempSummary)
#tempSummary

#temp Distribution
temp_plot <- CBS %>% ggplot(aes(temp)) +
  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = "blue", color = "black", alpha = 0.6) +
  geom_density(adjust = 3) +
  labs(x = "Normalized Celcius",
       y = "Density", 
       title = "Distribution of `temp`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))
```

```{r}
#Retrieve summary statistics for atemp
tempFCSummary = numSummary(CBS$atemp, "atemp")
numVars = bind_rows(numVars, tempFCSummary)
#tempFCSummary

#atemp Distribution
atemp_plot <- CBS %>% ggplot(aes(atemp)) +
  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = "blue", color = "black", alpha = 0.6) +
  geom_density(adjust = 3) +
  labs(x = "Normalized - Real Feel Celcius",
       y = "Density", 
       title = "Distribution of `atemp`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))
```

```{r}
#Retrieve summary statistics for humidity
humSummary = numSummary(CBS$hum, "hum")
numVars = bind_rows(numVars, humSummary)
#humSummary

#Humidity Distribution
hum_plot <- CBS %>% ggplot(aes(hum)) +
  geom_histogram(aes(y = after_stat(density)), fill = "blue", color = "black", alpha = 0.6) +
  geom_density(adjust = 3) +
  labs(x = "Normalized Humidity",
       y = "Density", 
       title = "Distribution of `hum`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))
```

```{r}
#Retrieve summary statistics for windspeed
windSummary = numSummary(CBS$windspeed, "windspeed")
numVars = bind_rows(numVars, windSummary)
#windSummary

#Windspeed Distribution
wind_plot <- CBS %>% ggplot(aes(windspeed)) +
  geom_histogram(aes(y = after_stat(density)), fill = "blue", color = "black", alpha = 0.6) +
  geom_density(adjust = 3) +
  labs(x = "Normalized Windspeed",
       y = "Density", 
       title = "Distribution of `windspeed`")  +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.x = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black"))
```

```{r}
#Summary Statistics of Continuous Variables 
library(DT)
datatable(numVars, class = 'cell-border stripe')
```

```{r fig.height=4, fig.width=5}
cont_plot <- ggarrange(temp_plot, atemp_plot, wind_plot, hum_plot)
cont_plot
```


##### `holiday`

Inspection of our `holiday` variable revealed dates which should have been marked as holidays and others which should not have been. We re-labeled these observations based on official [federally recognized holidays](https://www.commerce.gov/hr/employees/leave/holidays).

|       In comparison to our dataset: 2011-01-01, 2011-12-25, 2012-01-01, and 2012-11-11 were incorrectly mislabeled as *not* being holidays. 2011-12-26, 2012-01-02, and 2012-11-12 were mislabeled as **being** holidays. 2011-04-15 and 2012-04-16 are dates of observance for Emancipation Day in the Washington D.C. Area. Due to the holiday-like observance of Emancipation Day in our area of interest, these two dates will be labeled as holidays.

```{r}
holidayDates <- CBS[CBS$holiday == TRUE,]
holidayDates$date %>% unique()
```

```{r}
#2011 Holiday Dates
USFed2011 <- c("2011-01-01", "2011-01-17", "2011-02-21","2011-04-15", "2011-05-30", "2011-07-04", "2011-09-05", "2011-10-10", "2011-11-11", "2011-11-24", "2011-12-25")
incHolidayDates2011 <- CBS[CBS$date %in% USFed2011 & CBS$holiday == FALSE,]$date %>% unique()

#2012 Holiday Dates
USFed2012 <- c("2012-01-01", "2012-01-16", "2012-02-20", "2012-04-16", "2012-05-28", "2012-07-04", "2012-09-03", "2012-10-08", "2012-11-11", "2012-11-22", "2012-12-25")
incHolidayDates2012 <- CBS[CBS$date %in% USFed2012 & CBS$holiday == FALSE,]$date %>% unique()

#Merge incorrect dates
incHolidays <- c(incHolidayDates2011, incHolidayDates2012)
print("Should be labeled as holidays: ")
print(incHolidays) 

#Combined Holiday List 
combinedUSFedHolidays <- c(USFed2011, USFed2012)
print("Combined List of Federally Recognized U.S. Holidays for 2011/2012: ")
print(combinedUSFedHolidays)

#Correct Holiday Labels
CBS<- CBS %>% mutate(holiday = ifelse(date %in% c(combinedUSFedHolidays), TRUE, FALSE))
```

```{r fig.width=5}
print(paste("Percentages of holiday Observations:"))
(prop.table(table(CBS$holiday)) * 100) %>% round(2)

#Holiday Distribution
hol_plot <- CBS %>% 
  ggplot(mapping = aes(x = holiday, fill = holiday)) +
  geom_bar(color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 1000, size = 3) +
  scale_fill_viridis_d("Year", option = "magma") +
  labs(title = "Distribution of `holiday`",
       x = "",
       y = "") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  guides(fill = "none") +
  coord_flip()
```

```{r}
print(paste("Percentages of workingday Observations:"))
(prop.table(table(CBS$workingday)) * 100) %>% round(2)

#Holiday Distribution
work_plot <- CBS %>% 
  ggplot(mapping = aes(x = workingday, fill = workingday)) +
  geom_bar(color = "black") +
  geom_text(stat = "count", aes(label = after_stat(count)), nudge_y = 1000, size = 3) +
  scale_fill_viridis_d("Year", option = "magma") +
  labs(title = "Distribution of `workingday`",
       x = "",
       y = "") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  guides(fill = "none") +
  coord_flip()
```

```{r fig.width=4.2}
bool_plot <- ggarrange(work_plot, hol_plot,  nrow = 2, ncol = 1)
bool_plot
```




## Correlation

|       We decided to investigate the extent to which our variables are linearly related to one another with a visualized correlation matrix~[Figure XX]~. Here we are concerned with those relationships related to that of our target, `count_rentals` and continuous predictor variables. Here we list the most noteworthy findings:

-   Being that `temp` and `atemp` both aim to measure normalized Celsius, it is unsurprising that these variables exhibit a near exact linear relationship with one another. We additionally see they both are moderately, positively related to our target variable to the same degree. Naively including both `temp` and `atemp` in our modeling process would result in multicollinearity. To maintain a robust regression model and avoid multicollinearity which would cause our coefficients estimates to become unstable, we will choose only one of these variables, `atemp`, to be included in the modeling process.

-   Our target is slightly negatively correlated with humidity. Being that humidity includes potential precipitation such as rain, mist, snow, and others, this relationship is unsurprising.

|       Overall, we see that each of our continuous predictor variables exhibits straight-line relationship with our target variable. This can be seen by examining the bottom row of Figure XX as indicated by scatter-plots and the red trend lines.

```{r  fig.height=6, fig.width=6}
#Show relationships between predictors and dvs
pred_CBS <- CBS %>% dplyr::select(-c(registered, casual))
cor_matrix <- cor(pred_CBS[sapply(pred_CBS, is.numeric)])

# Visualize the correlation matrix using the corrplot package
corrplot(cor_matrix, method = "circle",
         type = "upper", order = "AOE", 
         tl.col = "black", tl.cex = 1,
         addCoef.col = "darkred",
         number.cex = 1, tl.srt = 50)
```

```{r  fig.height=8, fig.width=8}
library(PerformanceAnalytics)
corr_df <- pred_CBS %>% select_if(is.numeric)
chart.Correlation(corr_df)
```

## Notable Multivariate Analysis

### Temperature and Target Relationship

|       We examined the multi-variable relationship between our temperature related variables, `temp`, `atemp`, and `count_rentals`. We revealed twenty-four high-leverage observations having the exact same `atemp` value of 0.2424, but `temp` values which were relatively high and variable in relation, ranging from [0.62 to 0.86]. When considering the observational evidence of these twenty-four `temp` and `atemp` observations as being high leverage outliers, we have opted to remove these observations. This removal helps to ensure data integrity as we move towards the modeling process.

```{r fig.height=4.5, fig.width=5}
temp_mult_plot <- CBS %>% ggplot(aes(atemp, temp)) +
  geom_raster(aes(fill = count_rentals)) +
  labs(x = "Real Feel - Celcius",
       y = "Actual Temp - Celcius",
       title = "Rentals Relationship to Temperature Real & Feel",
       subtitle = "Highest Occurrance During Favorable Degrees of Celcius") +
  scale_fill_viridis_c(option = "magma") +
  theme(
    plot.title = element_text(hjust = 0, face = "bold", size = 12),
    axis.title.x = element_text(size = 8, color = "black"),
    axis.title.y = element_text(size = 8, color = "black"),
    axis.text.y = element_text(size = 8, color = "black")) +
  geom_rect(aes(xmin = .21, ymin = .55, xmax = .27, ymax = .94), color = "red", fill = NA) +
  geom_text(x = .20, y = .5, label = "Outliers", color = "red")
```

```{r fig.height=4.5, fig.width=5}
#Examine multi-variate outliers to find potential reason of high `temp` values
mvOutliers <- CBS[CBS$atemp < .3 & CBS$temp > .5,]
mvOutliers

#Remove Outlier Observations
subset <- (!(rownames(CBS) %in% rownames(mvOutliers)))
CBS <- CBS[subset, ]

rownames(CBS) <- NULL
```

### Work Commute - Hourly Rental Trends

|       Our intuition led us to investigate possible patterns of usage based solely on time. What we uncovered, showed that during working days, there is strong indication that bicycles are being utilized as a primary mode of transportation to and from work. Most notably, it is Capital Bike Shares `registered` rental users that are driving this trend. We see that between all rentals and  registered rentals, the trend is nearly identical~[Figure XX, Figure XX]~. Neither `casual` rentals nor rentals occurring on Saturday/Sunday exhibit this trend, adding further evidence to our speculation~[Figure XX, Figure XX]~. The bicycles may be being used as a primary mode of transportation for the majority of `registered` riders during to/from work. Around 8:00AM we note an upward trend of rentals and a decrease around 5:00PM/6:00PM. 

```{r fig.width=7, fig.height = 4}
#Examining the relationship between day of the week and hourly trends
hours <- CBS$hour %>% unique() %>% as.vector()

#Usage looks identical for casual & registered --> therefore only showing count_rentals
ss_plot <- CBS[(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), count_rentals, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Total Rentals",
       title = "Weekend Hourly Trend of Rentals")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 90,
                                   size = 9,
                                   vjust = .7),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)
```


```{r fig.width=7, fig.height = 8}
#Weekday Trend Over Hours - Casual
wk_cas <- CBS[!(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), casual, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Casual Rentals",
       title = "Weekday Hourly Trend of Casual Rentals")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 90,
                                   size = 9,
                                   vjust = .7),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)
```

```{r fig.width=7, fig.height = 10}
#Weekday Trend Over Hours - Registered
reg_plot <- CBS[!(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), registered, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Registered Rentals",
       title = "Weekday Hourly Trend of Registered Rentals",
       subtitle = "Peak Usage Before & After Work 9-5 Schedule")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 90,
                                   size = 9,
                                   vjust = .7),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)

#Weekday Trend Over Hours - Total
tot_plot <- CBS[!(CBS$day %in% c("Sunday", "Saturday")),] %>% 
  ggplot(mapping = aes(x = factor(hour, levels = hours), count_rentals, fill = year)) +
  geom_boxplot(alpha = 0.7, width = .8) +
  labs(x = "", y = "Total Rentals",
       title = "Weekday Hourly Trend of Total Rentals",
       subtitle = "Peak Usage Before & After Work 9-5 Schedule")+
  scale_fill_viridis_d(option = "magma") +
  theme(axis.text.x = element_text(angle = 90,
                                   size = 9,
                                   vjust = .7),
        axis.ticks = element_line(color = "black")) +
  facet_wrap(~day, ncol = 1, nrow  = 5)
```

## Feature Reduction

`date` - Our team reasoned that the creation of **731** distinct variables representing `date` could have severe impacts (high VIF and overfitting) on model performance. No unique information is provided by `date` that is not already found in our `year` and `month`, and `day` variable. To avoid these drawbacks, we have decided to disregard the `date` variable from the modeling process.  

|       Additionally, being that `instant` is merely an identifier for the observations, it will also be disregarded during the modeling process. Lastly, being that `count_rentals` is the exact sum of `casual` and `registered`, inclusion of these two would make our prediction analysis arbitrary. We likewise disregarded these from our modeling.

## Feature Engineering

Dummy Variable Creation - Categorical variables, such as `season`, `year`, `month`, `day`, `hour`, and `weather`, are better suited for dummy variable creation for use in our multiple linear regression model. This encoding ensures these factor/categorical variables are appropriately interpreted by our model.

```{r}
#Create Dummy Coding here for these variables 
library(fastDummies)

#Create Dummy Variables
CBS <- dummy_cols(CBS, select_columns = 
                    c("season", "year", "month", "day", "hour", "weather"),
                  remove_first_dummy = T) %>%
  dplyr::select(-c(season, year, month, day, hour, weather))

#Coerce Logical Types to Numeric
CBS <- CBS %>% mutate_if(is.logical, as.numeric)
CBS <- CBS %>% dplyr::select(-c("casual", "registered"))
CBS <- CBS %>% dplyr::select(-c(date, temp))
```

# Model Building

## Data Partition

|       With our goals centered on finding the most significant predictors with respect to determining the total number of hour-by-hour rentals, we decided to partition our data 80% towards training the model and 20% for testing model performance. Allowing a significant majority of our overall dataset to go towards the model fitting process enables us to get more precise, stable coefficient estimates.

```{r}
library(leaps) #Best Subset Selection
library(caret) #data split
#For Reproducibility
set.seed(123)
#80/20 Test Split 
index <- createDataPartition(CBS$count_rentals, p = .8, list = FALSE)
train <- CBS[index, ]
test <- CBS[-index, ]
rownames(train) <- 1:nrow(train)
rownames(test) <- 1:nrow(test)
```

## Initial Full Model(s)

|       Our initial, comprehensive full model encompasses all possible predictors within the Capital Bike Share dataset. Our approach allows us to gain a generalized sense as to the importance and effects our predictors are having in determining our response variable.

```{r, fig.width=6, fig.height=6}
full_model <- lm(count_rentals ~ ., data = train)
summary(full_model)
```


\begin{center}
Initial Full Model
\end{center}

$\hat{{count rentals}}$ = $\beta_0$ + $\beta_1$`holiday` + $\beta_2$`workingday` + $\beta_3$`atemp` + ... + $\beta_{52}$`weather_Type_4`

Where $\hat{{count rentals}}$ is the predicted number of rentals, $\beta_0$ is the intercept, and $\beta_1, \beta_2, ... \beta_n$ are the coefficients of the predictor variables.

|       While the full model containing all possible predictors can provide some insightful information, it is far from refined. Many of the predictors were found to be statistically insignificant in predicting the response. While we could remove these variables, we will allow step-wise selection to determine the best set of predictors in determining the response. To refine this full model, we will first inspect the assumptions of multiple linear regression.


### Model Diagnostics

#### Normality of Residuals

Our residuals should be normally distributed and can be visualized using a Q-Q plot. Deviations from our straight line in the diagnostic plot would suggest potential non-normality and is confirmed by the formal Kolmogorov-Smirnov Test. We can be certain that our residuals are not normally distributed.

```{r}
# Kolmogorov-Smirnov Test - Normality Assumption 
ks <- ks.test(full_model$residuals, 'pnorm')
if (ks$p.value <= 0.05) {
  result = paste("H0 rejected: Non-Normal residuals")
} else {
  result = paste("H0 failed to reject: the residuals ARE distributed normally")
}
ks
# Q-Q plot for residuals
qqnorm(resid(full_model))
qqline(resid(full_model))
```

#### Homoscedasticity

The homoscedasticity assumption states that we should have residuals ($\epsilon$) with a constant variance. The funnel-shape we see in our diagnostic plot is indicative of the opposite, heteroscedasticity, or non-constant variance, and is confirmed by our formal Breusch-Pagan Test. Our standard errors, confidence intervals, and subsequently our hypothesis testings rely on the homoscedasticity assumption. 


#### Linearity

The linearity assumption states that we should assume the true relationship between our predictors and response variable is a straight-line. We can identify non-linear trends with the red line fit to our residuals. Linearity appears to be violated here, as the upward-curved line is indicative of a non-linear relationship. 

```{r fig.height=3.5, fig.width=3.5}
#Breusch-Pagan Test - Homoscedasticity Assumption
library(lmtest)
bp <- bptest(full_model)
  if (bp$p.value <= 0.05) {
    result = paste("H0 rejected: Error variance spread INCONSTANTLY (Heteroscedasticity)")
  } else {
    result = paste("H0 failed to reject: Error variance spread CONSTANTLY (Homoscedasticity)")
  }
bp
print(result)
plot(full_model, which = 1)
```





#### Multicollinearity

The presense of multicollinearity reduces the accuracy in our model's coefficients by causing our coefficients' standard errors to grow, effectively masking their importance and making interpretation difficult. We can detect multicollinearity by calculating the variance inflation factor of our model's predictors. Our first full model contains a few predictors having VIF values greater than 10 which warrants possible removal from our model to remove multicollinearity.

```{r}
library(car)
df_vif = vif(full_model) %>% data.frame()
colnames(df_vif) = "VIF"
highest_vif <- df_vif %>% arrange(desc(VIF)) %>% head(8)
```

#### Independence of Errors

Our residuals should be independent of each other for our regression to be reliable. We can see from the previously mentioned Residuals vs. Fitted Figure that we have evidence of having violated this assumption and can confirm that indeed we have a lack independence of errors from the Durbin-Watson Formal Test. Our test result shows that indeed there is strong evidence of positive autocorrelation.

```{r fig.height=4, fig.width=4}
library(lmtest)
# Durbin-Watson Test
dwtest(full_model)
```

### Model Improvement

|       We want our final multiple linear regression model to be robust. To achieve this and bring our assumptions closer to be satisfied, we now proceed with various approaches of addressing issues which may be affecting our model's assumptions.

#### Remove Influential Observations

We begin by removing observations which we believe may be having too disproportionate an impact on our model's fit. The observations in particularly are likely to affect our model's coefficients, affecting our overall predictive accuracy. These include the three observations with significantly high cook's distances. Each removed observation were of `weather_type_4`. Because of this, the `weather_type_4` variable was consequently removed from the training data set.

```{r}
cd_plot <- plot(full_model, which = 5)
```

```{r fig.height=12, fig.width=12}
#Rownames were reset during train/test split
out_lev <- c(450, 7302, 7093)
train <- train[-c(out_lev), ]
train <- train %>% dplyr::select(-`weather_Type 4`)
```

#### Target Variable Transformation

We considered multiple transformations to approach normality and address the linearity and homoscedasticity assumptions, including, the Box-cox, logarithmic, square-root, cube-root, and fourth-root transformations. For each of our target variables, the transformation which approximated normality (skewness value to 0) the closest was the **cube-root** transformation. Therefore, we will predict the cube-root transformed version of our target variable `cube_root_total` for the final model.

```{r}
library(MASS)
#Box-cox Transformation on count_rentals
b <- boxcox(lm(CBS$count_rentals ~ 1), plotit = F, lambda = seq(-3,3, .05))
lambda <- b$x[which.max(b$y)]
box_total <- (CBS$count_rentals ^ lambda - 1) / lambda
#Log Transformation
log_total <- log(CBS$count_rentals)
#Sqrt Transformation
sqrt_total <- sqrt(CBS$count_rentals)
#Cube Root Transformation
cube_root_total <- (CBS$count_rentals)^(1/3)
#Fourth Root Transformation
fourth_root_total <- (CBS$count_rentals)^(1/4)

ggTrans <- function(var, myTitle) {
  #Cube Root Transformation - count_rentals
  skewVal <- skewness(var) %>% round(3)
  CBS %>% ggplot(aes(var)) +
    geom_histogram(aes(y = after_stat(density)),fill = "darkblue", color = "black", alpha = 0.6) +
    geom_density(adjust = 2) +
    labs(x = "Cube Root Total", y = "Density",
         title = myTitle,
         subtitle = paste("Skewness Value: ", skewVal)) +
    theme(
      plot.title = element_text(hjust = 0, face = "bold", size = 12),
      axis.title.x = element_text(size = 8, color = "black"),
      axis.title.y = element_text(size = 8, color = "black"),
      axis.text.x = element_text(size = 8, color = "black"),
      axis.text.y = element_text(size = 8, color = "black"))
}
```

```{r fig.height=8, fig.width=3}
#Inspect Distributions
pl1 <- ggTrans(cube_root_total, "Cube Root Transformation")
pl2 <- ggTrans(box_total, "Box-Cox Transformation")
pl3 <- ggTrans(fourth_root_total, "Fourth Root Transformation")
pl4 <- ggTrans(sqrt_total, "Square Root Transformation")
pl5 <- ggTrans(log_total, "Log Transformation")
trans_total <- ggarrange(pl1, pl2, pl3, pl4, pl5, nrow = 5, ncol = 1)
```

```{r}
#Add cube-root transformations to the dataset
train$cube_root_total <- (train$count_rentals)^(1/3)
test$cube_root_total <- (test$count_rentals)^(1/3)
```

#### Multicollinearity

We opted to remove the influences of multicollinearity in our models via removing the variables which had the highest variance inflation factors(VIF > 10) one at a time. The two variables that were removed were ultimately removed were `workingday`, and `season_Summer`.

```{r, fig.width=7, fig.height=7}
#Could remove workingday & season_Summer from train df
train <- train %>% dplyr::select(-c( workingday, season_Summer))
```

## Final Reduced Model Using Stepwise Selection

|       Using step-wise selection, we identified an optimal subset of predictors. These variables were determined to be the best set of predictors in predicting the transformed response, `cube_root_total`.

```{r fig.height=5, fig.width=8.5}
library(leaps)
OLS.regback <- leaps::regsubsets(cube_root_total ~ . - count_rentals, train, method = "seqrep", nvmax = 45)
OLSregbacksum <- summary(OLS.regback)
```

|       Our team believes that two of the best metrics for determining model fit are Adjusted $R^2$ and Bayesian Information Criterion (BIC). Both of these metrics help determine models which balance goodness-of-fit and complexity. We seek to maximize Adjusted $R^2$ and minimize BIC. Our step-wise selection found a final model which meets that criterion and can be seen in the Appendix as Figure Z with a visualization depicting the change in $R^2$ and BIC in Figure Z. 

```{r fig.height=8, fig.width=8.5}
adjr2mat <- data.frame(t(OLSregbacksum$adjr2))
bicmat <- data.frame(t(OLSregbacksum$bic))
cpmat <- data.frame(t(OLSregbacksum$cp))
rsqmat <- data.frame(t(OLSregbacksum$rsq))

combined_df <- data.frame(
  R2 = unlist(rsqmat),
  Adj_R2 = unlist(adjr2mat),
  BIC = unlist(bicmat),
  Mallow_Cp = unlist(cpmat)
)

adjr2 <- combined_df[36,]$Adj_R2 %>% round(2)
bic <- combined_df[36,]$BIC %>% round(2)
cp <- combined_df[36,]$Mallow_Cp %>% round(2)

par(mfrow = c(2,2))
plot(OLSregbacksum$adjr2, xlab = "Number of Variables",
    ylab = "Adjusted RSq", type = "l", lwd = 1.5, main = "Number of Variables Vs. Adjusted R2",
    sub = paste("Best Adjusted RSq Value: " , adjr2 ),
    cex.main = 1.15, cex.lab = 1, cex.axis = 1.05, font.axis = 2,
    font.lab = 2, panel.first = grid(nx = NULL, ny = NULL, col = "gray",
        lty = 2))
points(36, OLSregbacksum$adjr2[36], col = "darkred", cex = 2, pch = 20) 

plot(OLSregbacksum$cp, xlab = "Number of Variables", ylab = "Cp",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. Mallow's Cp",
    sub = paste("Best Mallow's Cp Value: " , cp ),
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
points(36, OLSregbacksum$cp[36], col = "darkred", cex = 2, pch = 20)

plot(OLSregbacksum$bic, xlab = "Number of Variables", ylab = "BIC",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. BIC",
            sub = paste("Best BIC Value: " , bic ),
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
points(36, OLSregbacksum$bic[36], col = "darkred", cex = 2, pch = 20)

plot(OLSregbacksum$rss, xlab = "Number of Variables", ylab = "RSS",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. RSS",
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
```

```{r}
# Find the model with the lowest BIC value
best_model_index <- which.min(OLSregbacksum$bic)

# Get the best model
best_model <- coef(OLS.regback, id = best_model_index)

# Extract the coefficients from the best model and remove the intercept term from the coefficients
best_coefficients <- round(unname(best_model)[-1], 4)

# Get the variable names from your redDF dataframe
variable_names <- names(best_model)[-1]

# Extract the intercept coefficient separately
intercept_coefficient <- round(unname(best_model)[1], 4)

# Define the named_coefficients variable
named_coefficients <- setNames(best_coefficients, variable_names)

# Create the LaTeX equation
equation <- paste("y =", intercept_coefficient, "+", paste(named_coefficients, variable_names, sep = " * ", collapse = " + "), "")
```

```{r}
#---Prepare Final Dataset---  
#Extract the best stepwise variables from train
final_mod_vars <- named_coefficients %>% names()
vars <- train %>% names()

#Fix "`" and then return list of appropriate vars
variables_for_selection = c()
for (i in 1:length(final_mod_vars)) {
  var_to_list = str_replace_all(final_mod_vars[i], "`", "" )
  variables_for_selection = c(variables_for_selection, var_to_list)
}

#Prepare Final Training Dataset Based on Selected Variables
train_final <- train %>% select_if(vars %in% variables_for_selection)
train_final <- train_final %>% bind_cols(train$cube_root_total)
colname = train_final %>% names()
train_final <- train_final %>% rename(cube_root_total = colname[length(colname)])
```

```{r}
#Train Final Model - Success (Same coef as original)
stepwise_model <- lm(cube_root_total ~ .  ,data = train_final)
summary(stepwise_model)
```

### Interpretation of Model Coefficients

|       From our step-wise model, the three variables with the most significant impact on the expected number of total rentals are `hour_5:00PM`, `hour_6:00PM`, and `hour_8:00AM`.

|       For instance, when our `hour_5:00PM` variable is 1(True), holding other variables constant, the cube root number of bicycle rentals is expected to increase by 4.22167. In other words, if we back-transform(apply cubic) our coefficient value for interpretability, the number of bicycle rentals is expected to increase by 75.24. This procedure of coefficient interpretation applies to all coefficients in our model. In the case of continuous variables, such as `atemp`, the expected increase in the number of bicycle rentals is determined by a one unit increase in the predictor value.

```{r fig.height=7, fig.width=6}
library(coefplot)
coefplot::coefplot(stepwise_model, sort = "magnitude", decreasing = TRUE) 
```

### Model Diagnostics

|       Our approaches to satisfy the assumptions mentioned during the first full model fitting were promptly addressed with our transformation of the response variable via a cube root transformation, removal of over-influential observations, and high variance inflation variables. The 

|       Despite our approach, we did observe some potential violations of regression assumptions in homoscedasticity, linearity, and normality/independence of residuals. The multicollinearity assumption was satisfied due to our removing of high variance inflation variables.  

|       In practice, however, the assumptions are rarely validated. After employing our corrective measures and observing the resulting vastly improved diagnostic plots, we remain confident in our model's predictive capabilities and generalizability. Our attempts at rectifying our assumptions helped to stabilize our coefficients, and ensure that our model contains variables meaningful in determining our target.

```{r fig.height=5, fig.width=5}
#Linearity-Homoscedasticity
plot(stepwise_model, which = 1, main = "Evaluation of Homoscedasticity and Linearity")

# Kolmogorov-Smirnov Test - Normality Assumption 
ks <- ks.test(stepwise_model$residuals, 'pnorm')
if (ks$p.value <= 0.05) {
  result = paste("H0 rejected: Non-Normal residuals")
} else {
  result = paste("H0 failed to reject: the residuals ARE distributed normally")
}
ks

# Q-Q plot for residuals (Normality of Residuals)
qqnorm(resid(stepwise_model))
qqline(resid(stepwise_model))

# Durbin-Watson Test (Independence of Residuals)
dwtest(stepwise_model)

#Multicollinearity
vif_final <- vif(stepwise_model) %>% data.frame()
vif_final <- vif_final %>% rename(Highest_VIF_Values = ".") %>% arrange(desc(Highest_VIF_Values))
vif_final %>% head(5)
```

### Test Set Evaluation - Model Performance  

|       Utilizing the final step-wise model determined above, we tested our model's performance by making predictions on the test set. Because we applied a cube-root transformation to our target variable `cube_root_total`, we **back-transform** these predicted/fitted values for interpretability. Doing so allows us to draw meaningful interpretation from our model's performance in the original units.  

|       We have provided the Root Mean Squared Error(RMSE), $R^2$, and Mean Absolute Error(MAE). RMSE and MAE are measured in the same units as the target variable and thus have a meaningful interpretation. MAE for example, is the "on-average" error between the predicted value and the true number of bike rentals, which for our model is approximately **63** rentals. Additionally, our $R^2$ value is approximately 0.734, meaning that 73.4% of the variability in the number of bike rentals is explained by our model which tells us that our model is a strong fit and is generalizable to new data predictions.

```{r}
#Make Predictions
pred_values_total_rentals <- predict(stepwise_model, newdata = test) %>% data.frame()
pred_values_total_rentals$fit = (pred_values_total_rentals$.)^3
pred_values_total_rentals$obs = (test$cube_root_total)^3


#Return Metrics
metrics <- function(y_pred, y_true){
 rmse <- RMSE(y_pred, y_true) %>% round(2)
 mse <- mean((y_pred - y_true)^2) %>% round(2)
 mae <- MAE(y_pred, y_true) %>% round(2)
 corPredAct <- cor(y_pred, y_true) %>% round(3)
 df = data.frame(Root_Mean_Squared_Error = rmse,
                 Mean_Absolute_Error = mae,
                 R_squared_Pred_Obs = (corPredAct*corPredAct) %>% round(3))
 df <- df %>% t() %>% data.frame()
 df <- df %>% rename(Metric = ".")
 return (df)
}
model_metrics_df = metrics(y_pred = pred_values_total_rentals$fit,
                           y_true = pred_values_total_rentals$obs)
```

```{r fig.height=5, fig.width=7}
#Total Rentals Predicts vs. Observed Values
a_v_p_plot <- ggplot(pred_values_total_rentals, aes(x=fit, y=obs)) +
  geom_point(aes(color = abs(obs - fit)), size = 3, alpha = 0.6) + 
  geom_abline(intercept=0, slope=1, linetype="dashed", color="red") +
  scale_color_viridis_c(option = "B", direction = -1) + 
  labs(
    title = "Actual vs Predicted Number of Total Rentals",
    subtitle = paste("Average Prediction Error(Mean Absolute Error): ", model_metrics_df$Metric[2], "bike rentals"),
    y = "Actual Number of Rentals",
    x = "Predicted Number of Rentals",
    color = "Residual Magnitude") +
  scale_x_continuous(breaks = seq(0, 1000, 250), labels = seq(0,1000,250)) +
   scale_y_continuous(breaks = seq(0, 1000, 250), labels = seq(0,1000,250)) +
  theme(legend.position = "bottom",
        axis.text= element_text(face = "bold",
                                   size = 12),
        plot.title = element_text(face = "bold",
                                  size = 16))
```

# Conclusions

|       The Capital Bike Share dataset analysis revealed insightful patterns and statistics in pursuit of variables which were used in formulating a reliable multiple linear regression model. Significant influences found when examining the distributions of features such as the hour of the day and real feel Celsius temperature, help us understand what drives the number of total rentals.

|       Our final model provides a concise selection of key predictors, allowing us to filter out non-meaningful features which would otherwise prove irrelevant. Our approach allows us to ultimately make accurate, and reliable predictions as to the number of Capital Bike Share rentals.

## Recommendations

|       Based on the hourly working day commute trends showing commutes to and from the workplace as being a major driving force in the number of rentals, and the extreme prevalence and significance of the hour variables present in our final model, we believe we have a strong recommendation for Capital Bike Share.

|       There can be no doubt that registered commuters are utilizing bicycles as their primary mode of transportation during workplace commutes. Capital Bike Share can capitalize on this, by installing/placing additional bicycle kiosks in key residential and traffic heavy work-place zones. Doing so would incentivize additional bike rentals by permitting commuters a reliable source of transportation and ultimately increasing the number of bicycle rentals.

\pagebreak

# Appendix

```{r include = FALSE, echo = FALSE, eval = FALSE}
#Visualize for Missing Data
vis_miss(CBS, sort_miss = T) + 
  labs(y = "NA Values") +
  theme(axis.text.x.bottom = element_text(angle = 60, vjust = 0)) +
  coord_flip()
```

```{r echo=FALSE, fig.align='center', fig.cap="No missing values in dataset", fig.height=3, fig.show='hold', fig.width=6, include=TRUE}
fig.1
```

```{r include = FALSE, echo = FALSE, eval = FALSE}
#Visualize Data Types
vis_dat(CBS) + theme(axis.title.y = element_blank(),
                     axis.text.x.bottom = element_blank()) +
  labs(title = "Data Types of Capital Bike Share Dataset") +
  scale_fill_brewer(palette = 2)
``` 


```{r echo=FALSE, fig.align='center', fig.cap="Pre-assigned Variable Data Types", fig.height=2.2, fig.show='hold', fig.width=6, include=TRUE}
fig.2
```

```{r echo=FALSE, fig.align='center', fig.cap="Reassigned Variable Data Types", fig.height=2.7, fig.show='hold', fig.width=6, include=TRUE}
datatype_plot
```

```{r echo=FALSE, fig.align='center', fig.cap="Distribution of Target Variable", fig.show='hold', fig.width=5, include=TRUE}
target_plot
```

```{r fig.height=4, fig.width=4.5, echo=FALSE, fig.align='center', fig.cap="Continuous Variables Distribution", fig.show='hold', include=TRUE}
cont_plot
```

```{r fig.height=4, fig.width=8.5, echo=FALSE, fig.align='center', fig.cap="Continuous Variables Summary Statistics", fig.show='hold', include=TRUE}
#Summary Statistics of Continuous Variables 
datatable(numVars, class = 'cell-border stripe', options = list(searching = FALSE,
                                                                dom = 't'))
```

\pagebreak

```{r fig.height=2.2, fig.width=4.5, echo=FALSE, fig.align='center', fig.cap="Boolean Variables Distribution", fig.show='hold', include=TRUE}
bool_plot
```


```{r fig.height=4, fig.width=8,  echo=FALSE, fig.align='center', fig.cap="Distributions of `season`, `year`, `month`, and `day`", fig.show='hold', include=TRUE}
cat_plot_1 <- ggarrange(season_plot, year_plot, month_plot, day_plot)
cat_plot_1
```


```{r fig.height=5, fig.width=7.5, echo=FALSE, fig.align='center', fig.cap="Distributions of `hour`, and `weather`", fig.show='hold', include=TRUE}
cat_plot_2 <- ggarrange(hour_plot, weather_plot, nrow = 2, ncol = 1)
cat_plot_2
```

```{r fig.height=3, fig.width=5,  fig.align='center', fig.cap="Dates with Low Number of Observations",  include=TRUE}
date_plot
```

```{r fig.height=6, fig.width=6,  fig.align='center', fig.cap="Correlation Matrix and Continuous Variables Relationship with Target Variable",  include=TRUE}
corr_mat <- chart.Correlation(corr_df)
corr_mat
```


```{r fig.height=4, fig.width=5,  fig.align='center', fig.cap="High Leverage Points Between `temp` and `atemp`",  include=TRUE}
temp_mult_plot
```

```{r fig.height=4, fig.width=5,  fig.align='center', fig.cap="Smooth Curve of Rental with Peak Usage Around Midday",  include=TRUE}
ss_plot
```


```{r fig.height=8, fig.width=6,  fig.align='center', fig.cap="Low Casual Usage Throughout Working Days",  include=TRUE}
wk_cas
```


```{r fig.height=8, fig.width=6,  fig.align='center', fig.cap="Registered Work Commute Usage",include=TRUE}
reg_plot
```


```{r fig.height=8, fig.width=6,  fig.align='center', fig.cap="Total Rentals Work Commute Usage", include=TRUE}
tot_plot
```



```{r fig.height=8, fig.width=5,  fig.align='center', fig.cap="Zero in Confidence Intervals - Insignificant Predictor", include=TRUE}
coefplot::coefplot(full_model, sort = "magnitude", decreasing = TRUE) + labs(title = "Initial Full Model Coefficients", x = "Coefficient Value", y = "Predictor")
```
\pagebreak 

```{r fig.height=5, fig.width=5,  fig.align='center', fig.cap="Normality of Residuals Assumption - Violated", echo = FALSE, include=TRUE}
# Kolmogorov-Smirnov Test - Normality Assumption 
ks.test(full_model$residuals, 'pnorm')
if (ks$p.value <= 0.05) {
  print("H0 rejected: Residuals are NOT normally distributed")
} else {
  print("H0 failed to reject: the residuals ARE distributed normally")
}

# Q-Q plot for residuals
qqnorm(resid(full_model))
qqline(resid(full_model))
``` 

```{r fig.height=5, fig.width=5,  fig.align='center', fig.cap="Homoscedasticity and Linearity Assumptions - Violated", echo = FALSE, include=TRUE}
#Breusch-Pagan Test - Homoscedasticity Assumption
bp <- bptest(full_model)
  if (bp$p.value <= 0.05) {
    print("H0 rejected: Error variance spread INCONSTANTLY (Heteroscedasticity)")
  } else {
    print("H0 failed to reject: Error variance spread CONSTANTLY (Homoscedasticity)")
  }


plot(full_model, which = 1)
```

\pagebreak 

```{r fig.cap="No Multicollinearity Assumption - Violated", echo = FALSE, include=TRUE}
print.noquote("Below: Highest Variance Inflation Factors of Full Model")
highest_vif
print.noquote("---")
```
\pagebreak 

```{r fig.height=5, fig.width=5,  fig.align='center', fig.cap="Influential Observations", echo = FALSE, include=TRUE}
plot(full_model, which = 5)
```

```{r fig.height=8, fig.width=5,  fig.align='center', fig.cap="Cube Root Transformation Closest Skewness Value to Zero", include=TRUE}
trans_total
```


\pagebreak 


\begin{center}
Final Model Equation
\end{center}


`cube_root_total` = $\beta_0$ + $\beta_1$`holiday` + $\beta_2$`atemp` + $\beta_3$`hum` + $\beta_4$`season_Spring` + $\beta_5$`season_Fall` + $\beta_6$`year_2012` + $\beta_7$`month_May` + $\beta_8$`month_Aug` + $\beta_9$`month_Sep` +$\beta_{10}$`day_Friday` + $\beta_{11}$`day_Saturday` + $\beta_{12}$`hour_1:00PM` + $\beta_{13}$`hour_10:00AM` + $\beta_{14}$`hour_10:00PM` + $\beta_{15}$`hour_11:00AM` + $\beta_{16}$`hour_11:00PM` + $\beta_{17}$`hour_12:00AM` + $\beta_{18}$`hour_12:00PM` + $\beta_{19}$`hour_2:00AM` + $\beta_{20}$`hour_2:00PM` + $\beta_{21}$`hour_3:00AM` + $\beta_{22}$`hour_3:00PM` + $\beta_{23}$`hour_4:00AM` + $\beta_{24}$`hour_4:00PM` + $\beta_{25}$`hour_5:00AM` + $\beta_{26}$`hour_5:00PM` + $\beta_{27}$`hour_6:00AM` + $\beta_{28}$`hour_6:00PM` + $\beta_{29}$`hour_7:00AM` + $\beta_{30}$`hour_7:00PM` + $\beta_{31}$`hour_8:00AM` + $\beta_{32}$`hour_8:00PM` + $\beta_{33}$`hour_9:00AM` + $\beta_{34}$`hour_9:00PM` + $\beta_{35}$`weather_Type_2` + $\beta_{36}$`weather_Type_3` + $\epsilon$


```{r fig.height=8, fig.width=4,  fig.align='center', fig.cap="Assumption Corrections and High Leverage Removal", echo = FALSE, include=TRUE}
print.noquote("Below: Variance Inflation Factors of Final Model")
vif(stepwise_model)
print.noquote("---")

# Kolmogorov-Smirnov Test - Normality Assumption 
ks.test(stepwise_model$residuals, 'pnorm')
bptest(stepwise_model)

# Q-Q plot for residuals
par(mfrow = c(3,1))
qqnorm(resid(stepwise_model))
qqline(resid(stepwise_model))
plot(stepwise_model, which = 1)
plot(stepwise_model, which = 5)
```

\pagebreak

```{r fig.height=10, fig.width=9,  fig.align='center', fig.cap="Stepwise Selection Model Evaluations(BIC/RSquared/Mallows Cp/RSS)", include=TRUE}
par(mfrow = c(2,2))
plot(OLSregbacksum$adjr2, xlab = "Number of Variables",
    ylab = "Adjusted RSq", type = "l", lwd = 1.5, main = "Number of Variables Vs. Adjusted R2",
    sub = paste("Best Adjusted RSq Value: " , adjr2 ),
    cex.main = 1.15, cex.lab = 1, cex.axis = 1.05, font.axis = 2,
    font.lab = 2, panel.first = grid(nx = NULL, ny = NULL, col = "gray",
        lty = 2))
points(36, OLSregbacksum$adjr2[36], col = "darkred", cex = 2, pch = 20) 

plot(OLSregbacksum$cp, xlab = "Number of Variables", ylab = "Cp",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. Mallow's Cp",
    sub = paste("Best Mallow's Cp Value: " , cp ),
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
points(36, OLSregbacksum$cp[36], col = "darkred", cex = 2, pch = 20)

plot(OLSregbacksum$bic, xlab = "Number of Variables", ylab = "BIC",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. BIC",
            sub = paste("Best BIC Value: " , bic ),
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
points(36, OLSregbacksum$bic[36], col = "darkred", cex = 2, pch = 20)

plot(OLSregbacksum$rss, xlab = "Number of Variables", ylab = "RSS",
    type = "l", lwd = 1.5, cex.main = 1.15, cex.lab = 1, cex.axis = 1.05,
    font.axis = 2, font.lab = 2, main = "Number of Variables Vs. RSS",
    panel.first = grid(nx = NULL, ny = NULL, col = "gray", lty = 2))
``` 

\pagebreak

```{r fig.height=8.5, fig.width=5.5,  fig.align='center', fig.cap="Predictors for Final Stepwise Model", include=TRUE}
coefplot::coefplot(stepwise_model, sort = "magnitude", decreasing = TRUE) + labs(title = "Final Stepwise Model Variable Coefficients", x = "Coefficient Value", y = "Predictor") 
```
\pagebreak  

```{r fig.height=9, fig.width=9,  fig.align='center', fig.cap="Predictions Using the Test Dataset", include=TRUE}
a_v_p_plot
```

\pagebreak  
